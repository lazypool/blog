---
layout: post
title: æŠ±è„¸ç¤¾åŒºï¼šTransformersï¼Œå¯åŠ¨ğŸ¥³
date: 2023-11-16 02:16:27
categories:
    - ğŸ”§ å·¥å…·ä½¿ç”¨
tags: [äººå·¥æ™ºèƒ½, Transformer] 
index_img: https://cdn.jsdelivr.net/gh/lazypool/blog-pics/animals/cat0.png
---

# Transformersï¼Œå¯åŠ¨

Huggingface åœ¨ 2023 å¹´å¤§æ”¾å¼‚å½©ï¼Œæˆä¸º AI é¢†åŸŸæœ€æ´»è·ƒçš„å¼€æºç½‘ç«™ï¼Œå‰é€”ä¸å¯é™é‡ã€‚

## ä¹¦å†™å‰è¨€

å·¥æ¬²å–„å…¶äº‹ï¼Œå¿…å…ˆåˆ©å…¶å™¨ã€‚

ç°å¦‚ä»Šï¼ŒTransformers å·²ç»è¢«å°è£…æˆä¸€ä¸ªç‹¬ç«‹çš„åŒ…ï¼Œç”± Huggingface ä»¥ç¤¾åŒºçš„å½¢å¼ç»´æŠ¤ã€‚

å­¦ä¹  Transformers ååˆ†é‡è¦ï¼ä½œä¸ºåœ¨å„ä¸ªé¢†åŸŸéœ¸æ¦œçš„å­˜åœ¨ï¼Œä»»ä½•ä¸ AI æ‰“äº¤é“çš„äººéƒ½ä¸å¯èƒ½ç»•å¼€å®ƒã€‚

è¢«å°è£…å¥½çš„ Transformers ååˆ†ä¾¿åˆ©ï¼ä¸‰è¡Œè®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼éšæ—¶éšåœ°å¯¼å‡ºï¼åœ¨å¤šä¸ªæ¡†æ¶é—´å…±äº«ï¼

Hugginface è¦†ç›–äº†ç»å¤§å¤šæ•°çš„ Transformers çš„è¡ç”Ÿæ¨¡å‹ï¼Œå¹¶ç»´æŠ¤äº†ä¸€ä¸ªè‰¯å¥½çš„å¼€å‘ç”Ÿæ€ Hubã€‚

æœ€é‡è¦çš„æ˜¯ï¼ŒHugginface æä¾›äº†ä¸€ä¸ªç®€æ˜æ˜“æ‡‚çš„[å®˜æ–¹æ–‡æ¡£](https://hugginface.co/docs/transformers/index)ã€‚

## å®‰è£…æ–¹æ³•

ä¸€åˆ‡ç¥è¯ï¼Œå§‹äºå®‰è£…ã€‚åƒå®‰è£…æ™®é€šçš„åŒ…é‚£æ ·å®‰è£… Transformer å³å¯ã€‚

é€šå¸¸æ¥è¯´ï¼Œéœ€è¦æ­é…æŸä¸ªæ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œæ¯”å¦‚ Pytorchã€‚

å½“ç„¶ï¼Œè‰¯å¥½çš„åŒ…ç®¡ç†æ–¹å¼æ˜¯å…ˆåˆ›å»ºä¸€ä¸ªè™šæ‹Ÿç¯å¢ƒã€‚

é€šå¸¸æˆ‘ä»¬æŠŠå®ƒæ”¾åœ¨é¡¹ç›®ç›®å½•ä¸‹çš„ .env éšè—æ–‡ä»¶ï¼Œå¹¶ç”¨ .gitignore å°†å…¶å¿½ç•¥ã€‚

```bash
python -m venv .env
echo ".env" >> .gitignore
```

ä¹‹ååœ¨è¯¥è™šæ‹Ÿç¯å¢ƒä¸­æ ¹æ®æˆ‘ä»¬çš„éœ€æ±‚å®‰è£…ã€‚

```bash
source .env/bin/activate
pip install 'transformers[torch]'
```

## æœ¬åœ°è®¾ç½®

ä»è¿œç¨‹å®æ—¶ä¸‹è½½æ¨¡å‹æ˜¯ä¸€ä¸ªå¾ˆç¾å¦™çš„æƒ³æ³•ï¼Œä½†å› ä¸ºè¯¸å¦‚ GFW ç­‰çš„åŸå› ï¼Œå®ƒå¹¶ä¸æ€»èƒ½å®ç°ã€‚

æ›´å¸¸è§çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éœ€è¦ç¼“å­˜è®¾ç½®å’Œç¦»çº¿æ¨¡å¼ã€‚

### ç¼“å­˜è®¾ç½®

é¢„è®­ç»ƒæ¨¡å‹ä¸‹è½½åé€šå¸¸è¢«æ”¾åˆ°äº† ~/.cache/huggingface/hub ä¸‹ã€‚

è¿™ä¸ªæ–‡ä»¶å¤¹æ˜¯é»˜è®¤çš„ï¼Œé™¤éç¯å¢ƒå˜é‡ TRANSFORMERS\_CACHE è¢«æŒ‡å®šã€‚

è‡ªç„¶åœ°ï¼Œæˆ‘ä»¬æƒ³åˆ°éœ€è¦ä¿®æ”¹ shell çš„ç¯å¢ƒå˜é‡ï¼š

1. HUGGINGFACE\_HUB or TRANSFORMERS\_CACHE
2. HF\_HOME
3. XDG\_CACHE\_HOME + /huggingface

Transformers ä¼šæŒ‰ç…§ä¼˜å…ˆçº§é€‰æ‹©å°†æ¨¡å‹ä¸‹è½½åˆ°ä»€ä¹ˆåœ°æ–¹ã€‚

ç”±äºæˆ‘ä»¬ä½¿ç”¨è™šæ‹Ÿç¯å¢ƒä¸‹å®‰è£…ï¼Œæ¨èçš„åšæ³•æ˜¯å°†ç¯å¢ƒå˜é‡çš„æŒ‡å®šæ·»åŠ åˆ° activate æ–‡ä»¶ä¸­ã€‚

```bash
echo "export TRANSFORMERS_CACHE=$(echo $VIRTUAL_ENV|sed 's/\.env/\.cache/')" >> .env/bin/activate
sed -i '/deactivate () {/a unset TRANSFORMERS_CACHE' .env/bin/activate
```

ä»¥ä¸Šå‘½ä»¤éœ€è¦åœ¨è™šæ‹Ÿç¯å¢ƒä¸‹æ‰§è¡Œã€‚

### ç¦»çº¿æ¨¡å¼

ä½œä¸º GFW å†…çš„å­æ°‘ï¼Œæˆ‘ä»¬æ›´å¤šçš„æƒ³è¦ä½¿ç”¨ç¦»çº¿æ¨¡å¼ã€‚

å…¶å®ç¦»çº¿æ¨¡å¼å°±æ˜¯å°†æ¨¡å‹ä¸‹è½½åˆ°æœ¬åœ°ç¼“å­˜èµ·æ¥ï¼Œç„¶åè°ƒç”¨ã€‚

éœ€è¦æŒ‡å®šç¯å¢ƒå˜é‡ TRANSFORMERS\_OFFLINE = 1 ä»¥å¼€å¯ã€‚

```bash
echo "export TRANSFORMERS_OFFLINE=1" >> .env/bin/activate
sed -i '/deactivate () {/a unset TRANSFORMERS_OFFLINE' .env/bin/activate
```

è¿™æ ·ä¸€æ¥å®ƒå°†ä¸ä¼šè¯•å›¾ä» Hub ä¸‹è½½æ–‡ä»¶ï¼Œç›¸åº”çš„ï¼Œä½ éœ€è¦åœ¨æœ¬åœ°å‡†å¤‡æ¨¡å‹æ–‡ä»¶ã€‚

æ‰€æœ‰æ–‡ä»¶éƒ½å¯ä»¥ä» [Hub](https://huggingface.co/models) ä¸‹è½½ï¼Œå®ƒä»¬æ˜¯æœ‰äººç»´æŠ¤çš„ã€‚

### ä¸‹è½½æ–‡ä»¶

è¦ä» Hub ä¸Šé€ä¸€ä¸‹è½½æ¨¡å‹æ–‡ä»¶æ˜¯æ„šè ¢çš„ã€‚

ä¼˜é›…çš„æ–¹å¼æ˜¯å€ŸåŠ©ç¬¬ä¸‰æ–¹å·¥å…· git æ¥ä¸‹è½½ã€‚

ä½†æ¨¡å‹æ–‡ä»¶ä¸€èˆ¬å¾ˆå¤§ï¼Œè€Œ git ä¿å­˜ diff çš„æ–¹å¼ä¼šä½¿å¾—ä»“åº“åŠ¨è¾„å‡ ç™¾ Gã€‚

å› æ­¤ï¼Œæˆ‘ä»¬éœ€è¦å…ˆå®‰è£… LFS å·¥å…·ï¼Œå®ƒæ˜¯ä¸ git é›†æˆçš„ã€‚

å…³äº git-lfs çš„å®‰è£…ä¸é…ç½®ï¼Œè¯·æŸ¥çœ‹ GitHub çš„[å®˜æ–¹æ–‡æ¡£](https://docs.github.com/zh/repositories/working-with-files/managing-large-files/about-git-large-file-storage)ã€‚

ä¸è¿‡éƒ¨åˆ† Linux å‘è¡Œç‰ˆå…·æœ‰æ›´åŠ ç®€å•ã€å®‰å…¨çš„å®‰è£…æ–¹å¼ã€‚

```bash
sudo pacman -S git-lfs
git lfs install
```

ä½¿ç”¨ git-lfs çš„å¥½å¤„å°±æ˜¯å¯ä»¥ä¸ç”¨æŠŠæ‰€æœ‰å¤§æ¨¡å‹æ–‡ä»¶éƒ½ä¸‹è½½ä¸‹æ¥ï¼Œè€Œåªéœ€è¦æ ¹æ®éœ€è¦ pull å³å¯ã€‚

è¿™é‡Œä»¥ BERT ä¸ºä¾‹ï¼Œé¦–å…ˆå¯ä»¥æŠŠæ‰€æœ‰çš„å°æ–‡ä»¶éƒ½ä¸‹è½½ä¸‹æ¥ã€‚

```bash
cd .cache
GIT_LFS_SKIP_SMUDGE=1 git clone https://huggingface.co/bert-base-uncased
```

æˆ‘ä»¬ä¼šå‘ç°æ‰€æœ‰æ ‡æœ‰ LFS çš„å¤§æ–‡ä»¶éƒ½è¢«å­˜å‚¨æˆäº†æŒ‡é’ˆæ–‡ä»¶ã€‚

## å¿«é€Ÿå¼€å§‹

### Pipeline å‡½æ•°

æµç¨‹å‡½æ•° Pipeline() æ˜¯å®˜æ–¹æä¾›çš„ä¸€ä¸ªæ‡’äººå‡½æ•°ï¼Œå¯ä»¥ä½¿ç”¨æœ€ç®€å•ã€æœ€ä¾¿åˆ©çš„æ–¹æ³•è¿…é€Ÿè°ƒç”¨ä¸€ä¸ªæ¨¡å‹åˆ°æŒ‡å®šä»»åŠ¡ã€‚

æ¯”å¦‚ï¼Œæˆ‘æƒ³è¿›è¡Œä¸€æ¬¡ç®€å•çš„æƒ…æ„Ÿåˆ†æä»»åŠ¡ã€‚

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")

classifier("We are very happy to show you the ğŸ¤— Transformers library.")
```
```output
[{'label': 'POSITIVE', 'score': 0.9998}]
```

å½“ç„¶ï¼ŒPipeline() è¿˜å¯ä»¥æŒ‡å®šæ¨¡å‹ã€æ•°æ®é›†å’Œ Tokenizerï¼Œå…·ä½“å¯ä»¥æŸ¥çœ‹å®˜æ–¹æ–‡æ¡£ã€‚

### AutoClasss ç±»

äº‹å®ä¸Šï¼ŒPipeline() æ˜¯å¯¹ AutoClass ä¸­å„ä¸ªç»„ä»¶å³éƒ¨åˆ†åŠŸèƒ½çš„é›†æˆã€‚

è€Œ AutoClass æ˜¯ transformers çš„åŸºæœ¬ç±»ï¼ŒåŒ…æ‹¬ Tokenizer ç±», Model ç±»å’Œ Configuration ç±»ã€‚

æ‰€æœ‰ç›¸å…³çš„ç±»éƒ½è¡ç”Ÿè‡ªè¿™ 3 ä¸ªç±»ï¼Œå®ƒä»¬éƒ½æœ‰ save\_pretrained() å’Œ from\_pretrained() æ–¹æ³•ã€‚

#### Tokenizer ç±»

åˆ†è¯å™¨ Tokenizer ç”¨äºå°†æ–‡æœ¬å¤„ç†æˆä¸€åˆ—æ•°å­—ä½œä¸ºæ¨¡å‹è¾“å…¥ã€‚

Tokenization çš„è¿‡ç¨‹é¡»éµå¾ªä¸€å®šè§„åˆ™ï¼Œå…·ä½“å‚è§[å®˜æ–¹æ€»ç»“](https://huggingface.co/docs/transformers/tokenizer_summary)ã€‚

Tokenizer é¡»ä¿æŒå’Œæ¨¡å‹ä¸€è‡´ã€‚è­¬å¦‚ï¼ŒBERT å°±éœ€è¦ä½¿ç”¨ BERT çš„ Tokenzierã€‚

```python
from transformers import AutoTokenizer

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
tokenizer = AutoTokenizer.from_pretrained(model_name)

encoding = tokenizer("We are very happy to show you the ğŸ¤— Transformers library.")
print(encoding)
```
```output
{'input_ids': [101, 11312, 10320, 12495, 19308, 10114, 11391, 10855, 10103, 100, 58263, 13299, 119, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

æŒ‰ç…§çº¦å®šï¼ŒTokenizer è¿”å›ä¸€ä¸ªåŒ…å«å¦‚ä¸‹å­—æ®µçš„å­—å…¸ï¼š

- input\_ids: token çš„æ•°å­—è¡¨ç¤ºã€‚
- token\_type\_ids: token å±äºå¥å­ A è¿˜æ˜¯å¥å­ B
- attention\_mask: è¡¨ç¤ºå“ªä¸€ä¸ª token éœ€è¦è¢«æ³¨æ„ã€‚

Tokenzier èƒ½å¤Ÿæ¥å—ä¸€ä¸ª list å¹¶è¿”å›ä¸€ä¸ª batchï¼Œåªéœ€æŒ‡å®šæ˜¯å¦å¡«å……å’Œæˆªæ–­ã€‚

```python
pt_batch = tokenizer(
    ["We are very happy to show you the ğŸ¤— Transformers library.", "We hope you don't hate it."],
    padding=True,
    truncation=True,
    max_length=512,
    return_tensors="pt",
)
```

#### Model ç±»

ä¸‹è½½ AutoModel å’Œä¸‹è½½ Tokenizer ä¸€æ ·ç®€å•ï¼Œåªæ˜¯ä½ éœ€è¦æ ¹æ®ä»»åŠ¡éœ€æ±‚é€‰æ‹©åˆé€‚çš„æ¨¡å‹ã€‚

è­¬å¦‚ï¼Œå¯¹äºæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ï¼Œä½ éœ€è¦ä¸‹è½½ AutoModelForSequenceClassificationã€‚

```python
from transformers import AutoModelForSequenceClassification

model_name = "nlptown/bert-base-multilingual-uncased-sentiment"
pt_model = AutoModelForSequenceClassification.from_pretrained(model_name)
```

ä½ å¯ä»¥å°† batch ç›´æ¥è¾“å…¥æ¨¡å‹ï¼Œåªéœ€ä¸º dictionary åŠ ä¸Š \*\* çš„ä¿®é¥°ç¬¦ã€‚

```python
pt_outputs = pt_model(**pt_batch)
```

æ¨¡å‹è¾“å‡ºäº†æœ€åçš„æ¿€æ´»å€¼ï¼Œåªéœ€å†å¥—ä¸€ä¸ª softmax å°±èƒ½åšåˆ†ç±»ã€‚

```python
from torch import nn

pt_predictions = nn.functional.softmax(pt_outputs.logits, ndim=-1)
print(pt_predictions)
```
```output
tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],
        [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)
```

#### Configuration ç±»

æ¨¡å‹çš„è®¾ç½®æ˜¯å¯ä»¥è¢«ä¿®æ”¹çš„ï¼ŒåŒ…æ‹¬ä¸€äº›è¶…å‚æ•°ã€‚

è¿™æ˜¯å› ä¸ºå½“ä½ ä»ä¸€ä¸ª Config ç±»åˆå§‹åŒ–æ¨¡å‹æ—¶ï¼Œå®ƒæ˜¯ä»å¤´å¼€å§‹çš„ã€‚

ä½ åªéœ€å¼•å…¥ AutoConfig ç±»å¹¶è°ƒç”¨ from\_pretrained() å‡½æ•°ã€‚

```python
from transformers import AutoConfig

my_config = AutoConfig.from_pretrained("distilbert-base-uncased", n_heads=12)
```

ç„¶åå°†å…¶é€šè¿‡ from\_config() å‡½æ•°è°ƒç”¨ã€‚

```python
from transformers import AutoModel

my_model = AutoModel.from_config(my_config)
```

### Save/From å‡½æ•°

æ¨¡å‹è®­ç»ƒå®Œåï¼Œé€šè¿‡è°ƒç”¨ tokenizer çš„ save\_pretrained() å‡½æ•°å³å¯ä¿å­˜ã€‚

```python
pt_save_directory = "./pt_save_pretrained"
tokenizer.save_pretrained(pt_save_directory)
pt_model.save_pretrained(pt_save_directory)
```

å¦‚éœ€å¤ç”¨æ¨¡å‹ï¼Œé€šè¿‡ from\_pretrained() å‡½æ•°å³å¯ã€‚è¿™ä¹Ÿæ˜¯ä»è¿œç¨‹ä¸‹è½½æ¨¡å‹çš„å‡½æ•°ã€‚

```python
tf_model = TFAutoModelForSequenceClassification.from_pretrained("./tf_save_pretrained")
```

## é¡¹ç›®å®ä¾‹

åœ¨è¿™é‡Œä»¥è¯­ä¹‰ç›¸ä¼¼åº¦åˆ†æä¸ºä¾‹ï¼Œåšä¸€ä¸ªç®€å•çš„æ¨¡å‹æ­å»ºä¸è®­ç»ƒæ¡ˆä¾‹ï¼Œé¢„è®­ç»ƒæ¨¡å‹é€‰ç”¨ BERTã€‚

### åŠ è½½æ•°æ®

æ•°æ®é›†é€‰ç”¨çš„æ˜¯ STSBenchmarkï¼Œæ˜¯ç»å…¸çš„è¯­ä¹‰ç›¸ä¼¼åº¦æ•°æ®é›†ï¼Œå¯ä»¥åœ¨[æ­¤å¤„](http://ixa2.si.ehu.es/stswiki/images/4/48/Stsbenchmark.tar.gz)è·å–ã€‚

STSBenchmark ä»¥ CSV ä¸ªæ ¼å¼å­˜å‚¨ã€‚å…¶ä¸­ï¼Œç¬¬ 4 åˆ—æ˜¯ä¸¤ä¸ªè¯­å¥çš„ç›¸ä¼¼åº¦ï¼Œä¸ºä» 0 åˆ° 5 çš„æµ®ç‚¹æ•°ã€‚ç¬¬ 5 åˆ—å’Œç¬¬ 6 åˆ—æ˜¯ä¸¤ä¸ªè‹±æ–‡è¯­å¥ã€‚

```csv
main-captions	MSRvid	2012test	0000	5.000	A man with a hard hat is dancing.	A man wearing a hard hat is dancing.
main-captions	MSRvid	2012test	0002	4.750	A young child is riding a horse.	A child is riding a horse.
main-captions	MSRvid	2012test	0003	5.000	A man is feeding a mouse to a snake.	The man is feeding a mouse to the snake.
```

#### Dataset

Pytorch é€šè¿‡ Dataset ç±»å’Œ DataLoader ç±»å¤„ç†æ•°æ®é›†å’ŒåŠ è½½æ ·æœ¬ã€‚åŒæ ·åœ°ï¼Œè¿™é‡Œæˆ‘ä»¬é¦–å…ˆç»§æ‰¿ Dataset ç±»æ„é€ è‡ªå®šä¹‰æ•°æ®é›†ï¼Œä»¥ç»„ç»‡æ ·æœ¬å’Œæ ‡ç­¾ã€‚

```python
from torch.utils.data import Dataset


class STS(Dataset):
    def __init__(self, data_file):
        self.data = self.load_data(data_file)

    def load_data(self, data_file):
        data = {}
        with open(data_file, "r") as file:
            for idx, line in enumerate(file):
                row = line.strip().split("\t")
                data[idx] = {"score": row[4], "sent1": row[5], "sent2": row[6]}
        return data

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx]


sts_dev = STS("data/stsbenchmark/sts-dev.csv")
print(sts_dev[0])
```
```output
{'score': '5.000', 'sent1': 'A man with a hard hat is dancing.', 'sent2': 'A man wearing a hard hat is dancing.'}
```

å¯ä»¥çœ‹åˆ°ï¼Œæˆ‘ä»¬ç¼–å†™çš„ STS ç±»æˆåŠŸè¯»å–äº†æ•°æ®é›†ã€‚

æ¯ä¸ªæ ·æœ¬ä»¥å­—å…¸çš„å½¢å¼ä¿å­˜ï¼Œåˆ†åˆ«ä»¥ score, sent1, sent2 ä¸ºé”®å­˜å‚¨ç›¸ä¼¼åº¦å’Œå¥å­å¯¹ã€‚

#### DataLoader

æˆ‘ä»¬éœ€è¦ DataLoader æŒ‰ batch åŠ è½½æ•°æ®ï¼Œå¹¶å°†æ ·æœ¬è½¬æ¢ä¸ºæ¨¡å‹å¯ä»¥æ¥å—çš„è¾“å…¥æ ¼å¼ã€‚

å¯¹äº NLP ä»»åŠ¡ï¼Œè¿™ä¸ªç¯èŠ‚å°±æ˜¯å°†æ¯ä¸ª batch ä¸­çš„æ–‡æœ¬æŒ‰ç…§é¢„è®­ç»ƒæ¨¡å‹çš„æ ¼å¼è¿›è¡Œç¼–ç ï¼Œå¹¶è¿›è¡Œå¡«å……å’Œæˆªæ–­æ“ä½œã€‚

```python
from torch.utils.data import DataLoader
from transformers import BertTokenizer


def collote_fn(batch_samples):
    batch_score, batch_sent1, batch_sent2 = [], [], []
    for sample in batch_samples:
        batch_score.append(float(sample["score"]))
        batch_sent1.append(sample["sent1"])
        batch_sent2.append(sample["sent2"])
    X = tokenizer(
        batch_sent1, batch_sent2, padding=True, truncation=True, return_tensors="pt"
    )
    y = torch.tensor(batch_score)
    return X, y


def build_dataloader():
    sts_train = STS("data/stsbenchmark/sts-train.csv")
    sts_test = STS("data/stsbenchmark/sts-test.csv")

    train_loader = DataLoader(sts_train, batch_size=4, shuffle=True, collate_fn=collote_fn)
    test_loader = DataLoader(sts_test, batch_size=4, shuffle=True, collate_fn=collote_fn)
    return train_loader, test_loader


train_loader, test_loader = build_dataloader()
batch_X, batch_y = next(iter(train_loader))
print('batch_X shape:', {k:v.shape for k,v in batch_X.items()})
print('batch_y shape:', batch_y.shape)
print(batch_X)
print(batch_y)
```
```output
batch_X shape: {'input_ids': torch.Size([4, 40]), 'token_type_ids': torch.Size([4, 40]), 'attention_mask': torch.Size([4, 40])}
batch_y shape: torch.Size([4])
{'input_ids': tensor(
		[[  101,  6304,  2008,  3477,  1996,  1002,  1015,  1010, 20636,  4211,
          7408,  2131, 22434,  2494,  2007,  2184,  5080,  7396,  3229, 15943,
          1012,   102,  7027, 20874,  2005, 22434,  2494,  2007,  2184,  5080,
          7396,  3229, 15943,  2003,  1002,  1015,  1010, 20636,  1012,   102],
        [  101,  6458,  1024,  2158,  8980,  2015,  7376,  2482,  4413,  2086,
          2101,   102,  2149,  2158,  8980,  2015,  2482,  4413,  2086,  2044,
         11933,   102,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  2416,  6077,  2770,  2006,  1996,  7525,  3509,  1012,   102,
          2416,  6077,  3788,  3875,  1996,  3509,  1012,   102,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0],
        [  101,  1996,  7726,  2231,  3090,  1011,  1011,  1996,  7726,  4517,
          2565,  3464,  9379,  1012,   102,  1996,  7726,  2231,  2758,  1996,
          4517,  2565,  2003,  9379,  1012,   102,     0,     0,     0,     0,
             0,     0,     0,     0,     0,     0,     0,     0,     0,     0]]), 
'token_type_ids': tensor(
		[[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 
'attention_mask': tensor(
		[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,
         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],
        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,
         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}
tensor([4.3330, 4.0000, 3.4000, 5.0000])
```

DataLoader æŒ‰ç…§æˆ‘ä»¬è®¾ç½®çš„ batch size æ¯æ¬¡å¯¹ 4 ä¸ªæ ·æœ¬è¿›è¡Œç¼–ç ã€‚

å¹¶ä¸”é€šè¿‡å¡«å……å’Œæˆªæ–­çš„æ“ä½œä½¿å¾—æ¯ä¸ªæ ·æœ¬çš„é•¿åº¦ç›¸åŒï¼Œå¡«å……ä½ç½® 0 å¹¶ä¸”ç›¸åº”çš„ attention\_mask ç½® 0ã€‚

è¿™é‡Œæˆ‘ä»¬é€‰æ‹©çš„æ˜¯ BERT çš„ç¼–ç å™¨ï¼Œå› æ­¤æ¯ä¸ªæ ·æœ¬éƒ½è¢«å¤„ç†æˆäº†"[CLS] sent1 [SEP] sent2 [SEP]" çš„å½¢å¼ã€‚

[CLS] å¯¹åº”çš„ input\_ids æ˜¯ 101, è€Œ [SEP] å¯¹åº”çš„ input\_ids æ˜¯ 102ã€‚

### æ„å»ºæ¨¡å‹

è¿™é‡Œå®ç°äº†ä¸€ä¸ªæå…¶ç®€å•çš„æ¨¡å‹ï¼Œä»–åªåœ¨é¢„è®­ç»ƒçš„ BRET ä¸Šæ·»åŠ äº†ä¸€ä¸ªå…¨è¿æ¥å±‚ã€‚

```python
from torch import nn
from transformers import BertModel, BertConfig


class BertForSTS(BertModel):
    def __init__(self, config):
        super().__init__(config)
        self.bert = BertModel(config, add_pooling_layer=False)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.classifier = nn.Linear(768, 1)
        self.post_init()

    def forward(self, x):
        bert_output = self.bert(**x)
        cls_vectors = bert_output.last_hidden_state[:, 0, :]
        cls_vectors = self.dropout(cls_vectors)
        logits = self.classifier(cls_vectors)
        return logits


def build_model():
    config = BertConfig.from_pretrained("bert-base-uncased")
    model = BertForSTS.from_pretrained("bert-base-uncased", config=config)
    return model
```

BERT é¦–å…ˆä¼šå°†è¾“å…¥ç¼–ç ä¸º 768 ç»´çš„å‘é‡ï¼Œä¹‹ååˆ©ç”¨ä¸€å±‚å…¨è¿æ¥å°† 768 ç»´æ˜ å°„æˆä¸€ä¸ªå®æ•°æ¥è¿›è¡Œå›å½’ã€‚

æ³¨æ„ï¼Œæ­¤æ—¶æˆ‘ä»¬çš„æ¨¡å‹æ˜¯ Transformers é¢„è®­ç»ƒæ¨¡å‹çš„å­ç±»ï¼Œå› æ­¤éœ€è¦é€šè¿‡é¢„ç½®çš„ from\_pretrained å‡½æ•°æ¥åŠ è½½æ¨¡å‹å‚æ•°ã€‚

è¿™ç§æ–¹å¼ä½¿å¾—æˆ‘ä»¬å¯ä»¥æ›´çµæ´»åœ°æ“ä½œæ¨¡å‹ç»†èŠ‚ï¼Œä¾‹å¦‚è¿™é‡Œ Dropout å±‚å°±å¯ä»¥ç›´æ¥åŠ è½½ BERT æ¨¡å‹è‡ªå¸¦çš„å‚æ•°å€¼ã€‚

ä¸ºäº†ç¡®ä¿æ¨¡å‹çš„è¾“å‡ºç¬¦åˆæˆ‘ä»¬çš„é¢„æœŸï¼Œæˆ‘ä»¬å°è¯•å°†ä¸€ä¸ª Batch çš„æ•°æ®é€å…¥æ¨¡å‹ã€‚

```python
model = build_model()
outputs = model(batch_X)
print(outputs.shape)
```
```output
torch.Size([4, 1])
```

æ¨¡å‹è¾“å‡ºäº† 4X1 çš„å‘é‡ï¼Œå…¶ä¸­ 4 æ˜¯ batch çš„å¤§å°ï¼Œ1 æ˜¯è¾“å‡ºçš„ç»´åº¦ï¼Œç¬¦åˆæˆ‘ä»¬çš„é¢„æœŸã€‚

### è®­ç»ƒæ¨¡å‹

#### è®­ç»ƒç»„ä»¶

æ€»çš„æ¥è¯´ï¼Œè®­ç»ƒæ¨¡å‹éœ€è¦ loss\_fnï¼ˆæŸå¤±å‡½æ•°ï¼‰ã€optimizerï¼ˆä¼˜åŒ–å™¨ï¼‰ã€lr\_schedulerï¼ˆå­¦ä¹ ç‡è°ƒæ•´å™¨ï¼‰ã€‚

å…¶ä¸­ï¼ŒæŸå¤±å‡½æ•°ç”¨äºè®¡ç®—æ¢¯åº¦ï¼Œä¼˜åŒ–å™¨ç”¨äºå¹³æ»‘æ¢¯åº¦ï¼Œå­¦ä¹ ç‡è°ƒæ•´å™¨ç”¨äºåœ¨è®­ç»ƒè¿‡ç¨‹ä¸­è°ƒæ•´å­¦ä¹ ç‡ã€‚

```python
import torch.nn as nn
from transformers import AdamW, get_scheduler


def loss_fn(pred, y):
    mse_loss = nn.MSELoss()
    loss = mse_loss(pred.squeeze(), y)
    return loss


learning_rate = 1e-5
optimizer = AdamW(model.parameters(), lr=learning_rate)

epoch_num = 3
lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=epoch_num*len(train_loader)
)
```

åœ¨è¿™é‡Œï¼ŒæŸå¤±å‡½æ•°ä½¿ç”¨äº† torch.nn æä¾›çš„å‡æ–¹è¯¯å·® MSEã€‚

ä¼˜åŒ–å™¨é€‰ç”¨äº† AdamW å¹¶æŒ‡å®šäº†å­¦ä¹ ç‡ä¸º learning\_rateï¼ˆå– 1e-5ï¼‰ã€‚

lr\_scheduler åˆ¶å®šäº†å­¦ä¹ ç‡åˆ† epoch\_num \* len(train\_loader) æ­¥è¿›è¡Œä¸‹é™ã€‚

### è®­ç»ƒå¾ªç¯

ä¹‹åå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œé€šè¿‡è°ƒç”¨ä¸Šè¿°çš„ 3 ä¸ªç»„ä»¶æ¥ä¼˜åŒ–æ¨¡å‹çš„å‚æ•°ã€‚

åŒæ—¶å€ŸåŠ© tqdm åšäº†ä¸€ä¸ªè¿›åº¦æ¡æ¥å®ç°è®­ç»ƒè¿‡ç¨‹çš„å¯è§†åŒ–ã€‚

```python
from tqdm.auto import tqdm


def train_loop(dataloader, model, loss_fn, optimizer, lr_scheduler, epoch, total_loss):
    progress_bar = tqdm(range(len(dataloader)))
    progress_bar.set_description(f'loss: {0:>7f}')
    finish_step_num = (epoch-1)*len(dataloader)
    
    model.train()
    for step, (X, y) in enumerate(dataloader, start=1):
        pred = model(X)
        loss = loss_fn(pred, y)

        # é¦–å…ˆæ¸…é™¤ä¼˜åŒ–å™¨å·²æœ‰çš„æ¢¯åº¦
        optimizer.zero_grad()

        # è¿›è¡Œè¯¯å·®çš„åå‘ä¼ æ’­
        loss.backward()

        # æ›´æ–°å‚æ•°
        optimizer.step()

        # æ›´æ–°ä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡
        lr_scheduler.step()

        total_loss += loss.item()
        progress_bar.set_description(f'loss: {total_loss/(finish_step_num + step):>7f}')
        progress_bar.update(1)
    return total_loss


def test_loop(dataloader, model, mode='Test'):
    assert mode in ['Valid', 'Test']
    size = len(dataloader.dataset)
    correct = 0

    model.eval()
    with torch.no_grad():
        for X, y in dataloader:
            pred = model(X)
            correct += (pred - y).sum().item()

    correct /= size
    print(f"{mode} Accuracy: {(100*correct):>0.1f}%\n")
```

åœ¨è®­ç»ƒæ¨¡å‹æ—¶ï¼Œæˆ‘ä»¬å°†æ¯ä¸€è½® Epoch åˆ†ä¸ºè®­ç»ƒå¾ªç¯å’ŒéªŒè¯/æµ‹è¯•å¾ªç¯ã€‚

åœ¨è®­ç»ƒå¾ªç¯ä¸­è®¡ç®—æŸå¤±ã€ä¼˜åŒ–æ¨¡å‹çš„å‚æ•°ï¼Œåœ¨éªŒè¯/æµ‹è¯•å¾ªç¯ä¸­è¯„ä¼°æ¨¡å‹çš„æ€§èƒ½

æœ€åï¼Œå°†â€è®­ç»ƒå¾ªç¯â€å’Œâ€éªŒè¯/æµ‹è¯•å¾ªç¯â€ç»„åˆæˆ Epochï¼Œå°±å¯ä»¥è¿›è¡Œæ¨¡å‹çš„è®­ç»ƒå’ŒéªŒè¯äº†ã€‚

```python
total_loss = 0
for t in range(epoch_num):
    print(f"Epoch {t+1}/{epoch_num}\n-------------------------------")
    total_loss = train_loop(train_loader, model, loss_fn, optimizer, lr_scheduler, t+1, total_loss)
    test_loop(test_loader, model, mode='Test')
print("Done!")
```

å®Œæ•´çš„è®­ç»ƒè¿‡ç¨‹åº”å½“å¦‚ä¸‹ã€‚

```output
Epoch 1/3
-------------------------------
loss: 0.552296: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8584/8584 [07:16<00:00, 19.65it/s]
Valid Accuracy: 72.1%

Epoch 2/3
-------------------------------
loss: 0.501410: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8584/8584 [07:16<00:00, 19.66it/s]
Valid Accuracy: 73.0%

Epoch 3/3
-------------------------------
loss: 0.450708: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8584/8584 [07:15<00:00, 19.70it/s]
Valid Accuracy: 74.1%

Done!
```

å¤§åŠŸå‘Šæˆï¼ä½ å·²ç»æˆåŠŸæ„å»ºå¹¶è®­ç»ƒäº†ä¸€ä¸ªè¯­è¨€æ¨¡å‹ã€‚

## èµ„æ–™ä¸‹è½½

- STSBenchmark æ•°æ®é›†ï¼š[ç‚¹å‡»ä¸‹è½½](https://cdn.jsdelivr.net/gh/lazypool/blog-pics/blogpost/before2024/20231116_Stsbenchmark.tar.gz)
- æ¡ˆä¾‹ä»£ç ï¼š[ç‚¹å‡»ä¸‹è½½](https://cdn.jsdelivr.net/gh/lazypool/blog-pics/blogpost/before2024/20231116_example.tar.gz)
