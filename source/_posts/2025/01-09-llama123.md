---
layout: post
title: Llama1-3：从一道美团大模型面试题讲起🦘
date: 2025-01-09 20:25:53
categories:
    - 📖 论文阅读
tags: [大语言模型, NLP, NLP经典论文]
index_img: https://cdn.jsdelivr.net/gh/lazypool/blog-pics/animals/panda.png
---

# 从 LLaMA1 到 LLaMA3：这风云激荡的 2023 到 2024 年

这篇博客应当是我在 2025 年的首篇博客。24 年底，在我身上发生了很多意想不到的事情，总使我灰心丧气。忽然间，发觉自己除开写了十几个月的日记，并没有更多新的更新。前不久，在 B 站上看到名为 **美团大模型面试真题：LLaMA怎么优化注意力机制计算？** 的视频，我才惊觉时光飞逝。

于是想着，似乎应当写一篇详解 LLaMA 的博客出来。等到调查资料的时候，发现 LLaMA 原来早在 2023 年就已发布，且已到第三代。估计第四代就将在 2025 年发布吧？看来这篇博客实在是拖延不得了！
`（又：长久以来我总计划开设“NLP 经典论文”专区，整理自 2017 年以来“重点文”，不妨就以此篇作始吧！）`

## 2023 到 2024 年：从无到有，由弱及强

2023 到 2024 年，metaAI 先后发布 LLaMA1 到 LLaMA3 的论文。LLaMA 系列从无到有、由弱及强，成功跻身大语言模型先锋军之中。2023 也由此可以称作 **LLaMA 元年**。

- 2023.2.27  《LLaMA：开放高效的语言模型系列》 
- 2023.7.19  《LLaMA2：开放且经过微调的聊天模型》
- 2024.11.23 《LLaMA3 系列模型》

<table><thead>
    <tr><th rowspan="2">版本</th><th colspan="4" align="center">模型结构</th><th colspan="3" align="center">训练数据</th></tr>
    <tr><th>规范化</th><th>激活函数</th><th>位置编码</th><th>GQA</th><th>Tokens</th><th>上下文长度</th><th>Vocab Size</th></tr>
</thead><tbody>
    <tr><th>LLaMA1</th><th>RMSNorm</th><th>SwiGLU</th><th>RoPE</th><th>✗</th><th>1.4T</th><th>2K</th><th>32K</th></tr>
    <tr><th>LLaMA2</th><th>RMSNorm</th><th>SwiGLU</th><th>RoPE</th><th>✓</th><th>2.0T</th><th>4K</th><th>32K</th></tr>
    <tr><th>LLaMA3</th><th>RMSNorm</th><th>SiLU</th><th>RoPE</th><th>✓</th><th>15.0T</th><th>8K</th><th>128K</th></tr>
</tbody></table>

在模型结构上，LLaMA 自 1 代至 3 代总体改动较小，其中比较关键的技术是：**RMSNorm**、**RoPE** 和 **GQA**。**LLaMA 的迭代更新主要得力于 metaAI 不断扩充的优质语料库：**

- 自 1 代到 3 代，语料库的总 Token 量从 1.4T 上升至 15T；
- 词汇表大小也从最初的 32K 扩充至 128K；
- 预训练的上下文长度从 2K 增加到 4K，再到现在的 8K。

在模型参数量上，三代 LLaMA 总体呈现上升趋势：

- LLaMA1 分别具有 7B、13B、33B、65B 等版本；
- LLaMA2 与 1 代比差距不大，包括 7B、13B、34B、70B 等；
- LLaMA3 的参数量则显著提升，主要是 8B、70B 甚至 405B 等。

## LLaMA 是如何优化注意力机制的？

回到我们的话题上来：**LLaMA 是如何优化注意力机制的？** 事实上，LLaMA 在模型架构上大体仍沿用 _(Vanilla,2017)_ 提出的 Transformer，而进行了许多微小的改动。我根据自身对原论文的理解，认为其中比较重要的三个技术是：**RMSNorm**、**RoPE** 和 **GQA**。这些技术是什么意思？它们为什么能优化注意力机制？接下来对其进行详细讲解。

### RMSNorm：在不牺牲模型稳定性的同时降低计算复杂度

原始的 Transformer 使用的是 **层规范化** _(LayerNorm)_ 。之所以引入规范化，是希望模型中间层的输入分布始终保持稳定。具体来说，它对输入 $x$（假设 $x = [s_{0}^{j}, s_{1}^{j}, ..., s_{N}^{j}]$，$s$ 是长度为 $N$ 的序列，每个词向量取第 $j$ 个维度）进行如下操作：

$$y = \frac{x - E[x]}{\sqrt{Var[x] + \epsilon}} * \gamma + \beta. \quad \text{其中：} E[x] = \frac{1}{N} \sum_{i=1}^{N} x_i, \quad Var[x] = \frac{1}{N} \sum_{i=1}^{N} (x_i - E[x])^2$$

而 RMSNorm，或称 **均方根规范化** 就是 LayerNorm 的变体，RMSNorm 省去了求均值的过程，也没有了偏置 $\beta$：

$$y = \frac{x}{\sqrt{Mean(x^2) + \epsilon}} * \gamma. \quad \text{其中：} Mean(x^2) = \frac{1}{N} \sum_{i=1}^{N} x_i^2$$

首先，在计算上：RMSNorm 相比 LayerNorm 更为简便。其次，在效果上：容易看出 RMSNorm 具有对输入缩放的不变性（$y(ax) = y(x)$），一定程度上也能够稳定输入分布。最后，RMSNorm 并不保证输出的均值为 0。实验证明，该性质对模型的训练影响较小。

### RoPE：以简便方式有效捕捉相对位置信息

LLaMA 在每个 Attention 层中分别对 $Q$ 和 $K$ 进行 **旋转位置编码** _(RoPE, Rotary Postion Embedding)_ 具体来说，是在 $Q$ 和 $K$ 点积之前对 $Q$ 和 $K$ 分别编码。以下是部分 metaAI 官方给出的代码：

```python
class Attention(nn.Module):
    def forward(self, x, ...):
        bsz, seqlen, hidden_dim = x.shape
        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)

        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)
        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)
        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)

        # 对 Q 和 K 施用旋转位置编码
        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)
        
        ... # 此处省略部分代码

        # 对 Q 和 K 求点积并缩放
        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
```

那么，什么是 **旋转位置编码** ？RoPE 原论文的说法是：“在RoPE中，我们的出发点就是 **通过绝对位置编码的方式实现相对位置编码** 。”也即，我们考虑：

- 假设我们有编码操作 $f(·)$，分别对 $Q$ 的第 $m$ 维向量 $q$ 和 $K$ 的第 $n$ 维向量 $k$ 进行位置编码：
    - $q_m = f (q, m) \quad k_n = f (k, n)$
- 那么在 Attention 中对 $Q$ 和 $K$ 求点积时，我们希望 $q_m$ 和 $k_n$ 的内积结果将带入 $m-n$ 这个 **相对位置信息** _(m-n 将影响内积结果)_ ：
    - $<f (q, m), f (k, n)> = g(q, k, m-n)$
- 原论文经过复实变换得到了操作 $f(·)$ 的解：
    - $f(q, m) = q e^{i m \theta} = q \cdot (\cos m \theta+ i \sin m \theta)$. 其中 $i$ 为虚数单位，$\theta$ 为给定超参。
    - 根据复数乘法的几何意义，该变换实际上对应着向量的旋转（我们可以想象二维向量）。
- 最后，我们可以将 $f(q, m)$ 的完整形式表现如下（假设 $q$ 为 $d$ 维向量）：
    - 其中，$\theta_i = 10000^{-2i/d}$，这带来一定的远程衰减性。
    - $\otimes$ 是逐位对应相乘，即Numpy、Tensorflow等计算框架中的 $∗$ 运算。
    - 从这个实现也可以看到，RoPE可以视为是乘性位置编码的变体。

<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mtable displaystyle="true"><mlabeledtr><mtd></mtd><mtd><mrow><mo>(</mo><mtable rowspacing="4pt" columnspacing="1em"><mtr><mtd><msub><mi>q</mi><mn>0</mn></msub></mtd></mtr><mtr><mtd><msub><mi>q</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>q</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd><msub><mi>q</mi><mn>3</mn></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><msub><mi>q</mi><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mo>−</mo><mn>2</mn></mrow></msub></mtd></mtr><mtr><mtd><msub><mi>q</mi><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msub></mtd></mtr></mtable><mo>)</mo></mrow><mo>⊗</mo><mrow><mo>(</mo><mtable rowspacing="4pt" columnspacing="1em"><mtr><mtd><mi>cos</mi><mo>⁡</mo><mi>m</mi><msub><mi>θ</mi><mn>0</mn></msub></mtd></mtr><mtr><mtd><mi>cos</mi><mo>⁡</mo><mi>m</mi><msub><mi>θ</mi><mn>0</mn></msub></mtd></mtr><mtr><mtd><mi>cos</mi><mo>⁡</mo><mi>m</mi><msub><mi>θ</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><mi>cos</mi><mo>⁡</mo><mi>m</mi><msub><mi>θ</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>cos</mi><mo>⁡</mo><mi>m</mi><msub><mi>θ</mi><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>2</mn><mo>−</mo><mn>1</mn></mrow></msub></mtd></mtr><mtr><mtd><mi>cos</mi><mo>⁡</mo><mi>m</mi><msub><mi>θ</mi><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>2</mn><mo>−</mo><mn>1</mn></mrow></msub></mtd></mtr></mtable><mo>)</mo></mrow><mo>+</mo><mrow><mo>(</mo><mtable rowspacing="4pt" columnspacing="1em"><mtr><mtd><mo>−</mo><msub><mi>q</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><msub><mi>q</mi><mn>0</mn></msub></mtd></mtr><mtr><mtd><mo>−</mo><msub><mi>q</mi><mn>3</mn></msub></mtd></mtr><mtr><mtd><msub><mi>q</mi><mn>2</mn></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mo>−</mo><msub><mi>q</mi><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mo>−</mo><mn>1</mn></mrow></msub></mtd></mtr><mtr><mtd><msub><mi>q</mi><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mo>−</mo><mn>2</mn></mrow></msub></mtd></mtr></mtable><mo>)</mo></mrow><mo>⊗</mo><mrow><mo>(</mo><mtable rowspacing="4pt" columnspacing="1em"><mtr><mtd><mi>sin</mi><mo>⁡</mo><mi>m</mi><msub><mi>θ</mi><mn>0</mn></msub></mtd></mtr><mtr><mtd><mi>sin</mi><mo>⁡</mo><mi>m</mi><msub><mi>θ</mi><mn>0</mn></msub></mtd></mtr><mtr><mtd><mi>sin</mi><mo>⁡</mo><mi>m</mi><msub><mi>θ</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><mi>sin</mi><mo>⁡</mo><mi>m</mi><msub><mi>θ</mi><mn>1</mn></msub></mtd></mtr><mtr><mtd><mo>⋮</mo></mtd></mtr><mtr><mtd><mi>sin</mi><mo>⁡</mo><mi>m</mi><msub><mi>θ</mi><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>2</mn><mo>−</mo><mn>1</mn></mrow></msub></mtd></mtr><mtr><mtd><mi>sin</mi><mo>⁡</mo><mi>m</mi><msub><mi>θ</mi><mrow class="MJX-TeXAtom-ORD"><mi>d</mi><mrow class="MJX-TeXAtom-ORD"><mo>/</mo></mrow><mn>2</mn><mo>−</mo><mn>1</mn></mrow></msub></mtd></mtr></mtable><mo>)</mo></mrow></mtd></mlabeledtr></mtable></math>

最后在这里贴一份 metaAI 官方写的代码，以便更加直观地看到 RoPE 操作是如何实现的。

```python
def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):
    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))
    t = torch.arange(end, device=freqs.device)  # type: ignore
    freqs = torch.outer(t, freqs).float()  # type: ignore
    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64
    return freqs_cis

def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):
    ndim = x.ndim
    assert 0 <= 1 < ndim
    assert freqs_cis.shape == (x.shape[1], x.shape[-1])
    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]
    return freqs_cis.view(*shape)

def apply_rotary_emb(xq: torch.Tensor, xk: torch.Tensor, freqs_cis: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))
    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)
    return xq_out.type_as(xq), xk_out.type_as(xk)
```

### GQA：减少 KV-Cache 大小以提升模型推理速度

#### 引入 GQA 前先简单介绍下 KV-Cache

提高大模型推理速度的常用关键技术是 **KV-Cache**。大模型以 **自回归** _(auto regressive)_ 的方式推理，预测下个 token 时，总是基于之前已经生成的序列。从代码的角度来理解是这样的：

```python
inputs = "这是初始的输入序列"

while not stop:
    # 根据之前的序列预测下个 token
    next_token = model(inputs)

    # 如果下个 token 是休止符，退出
    # 否则，将 token 加入序列进行下次预测
    if next_token == '[SEP]':
        stop = True
    else:
        inputs += next_token

outputs = inputs
```

显然，随着输入序列越来越长，每次都重新计算所有 token 注意力值的传统方法显得非常低效。针对该问题，KV-Cache 采用 **空间换时间** 的方法：在自回归过程中，开辟空间将每次计算的 $K$ 和 $V$ 缓存起来，这样在处理新序列时只需从缓存中读取之前的 $K$ 和 $V$，无需重复计算。LLaMA 中的代码实现如下：

```python
class Attention(nn.Module):
    def __init__(self, ...):
        super().__init__()
        ... # 此处省略部分代码

        # 开辟 cache_k 用于缓存 k
        self.cache_k = torch.zeros(
            (
                args.max_batch_size,
                args.max_seq_len,
                self.n_local_kv_heads,
                self.head_dim,
            )
        ).cuda()
        # 开辟 cache_v 用于缓存 v
        self.cache_v = torch.zeros(
            (
                args.max_batch_size,
                args.max_seq_len,
                self.n_local_kv_heads,
                self.head_dim,
            )
        ).cuda()

    def forward(self, x, ...):
        ... # 此处省略部分代码

        # 将缓存置于 xq 所在的计算单元（CPU 或 CUDA）
        self.cache_k = self.cache_k.to(xq)
        self.cache_v = self.cache_v.to(xq)

        # 将当前计算出的 K 与 V 缓存起来
        # 缓存位置是从 start_pos 到 start_pos + seqlen
        # start_pos 是上次缓存后的序列长度
        # seqlen 是此次新增的子序列长度，KV-Cache 并非逐字缓存的
        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk
        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv

        # 获取缓存的 K 与 V，用于之后的计算
        keys = self.cache_k[:bsz, : start_pos + seqlen]
        values = self.cache_v[:bsz, : start_pos + seqlen]
```

在了解了 KV-Cache 后，我们不禁会反思：既然 $K$ 和 $V$ 能够缓存，那为什么不缓存 $Q$ 呢？答案是：不需要！与 BERT 不同，LLaMA、GPT 等大语言模型都是 Decoder-Only 架构，采用 **单向注意力机制**。由于使用了注意力掩码，位置靠后 token 的 $K$ 值将无法与位置靠前 token 的 $Q$ 值求内积。因此，缓存历史 token 的 $Q$ 值是完全没有必要的。

#### GQA：用分组的方式减少 KV 对的数量

然而，引入 KV-Cahe 意味着需要开辟大量的缓存空间。在 **多头注意力** _(MHA)_ 中，所需开辟的缓存空间大小可按照如下公式计算：

$$N \times B \times L \times H \times D$$

- $N$ 指 Transformer 块的数量。以 LLaMA2 为例，它共包含 32 个。
- $B$ 指输入数据的批量大小。这里我们假设是 16。
- $L$ 为输入句子的长度。为便于计算，我们取最大长度为 1024。
- $H$ 是注意力头的个数。假设是 32，与原论文保持一致。
- $D$ 是隐藏层的维度数。它在 LLaMA-7B 中被设为 4096。

根据公式，我们得出为了缓存 KV 所需的空间大小为 $32 * 16 * 1024 * 32 * 4096 = 2^{36}$ 份浮点数所需的空间。若我们使用半精度浮点数 float16 类型存储，就需要 $2^{36} * 2 \text{Byte} = 128 \text{GB}$ 大小的存储空间！参照 LLaMA1 论文中给出的硬件设置，这远远地超出了他们所提供的 RAM 80GB 的容量。

为解决这一问题，我们必须优化算法。由此，便引出了 LLaMA 自 2 代开始所使用的 **GQA** _(Group Query Attention)_ 。以下摘自 GQA 的原论文的图片简单明了地说明了什么是 GQA。

![MHA GQA MQA](https://cdn.jsdelivr.net/gh/lazypool/blog-pics/blogpost/the2025/0109_mha_gqa_mqa.png)

如图所示：在 **多头注意力机制（左，MHA）** 中，每个注意力查询 Q 都有各自对应的 KV 对，这导致缓存 KV 所需的空间极大。最极端的情况下，如 **多查询注意力机制（右，MQA）**，所有的查询 Q 共享唯一的 KV 对，虽最大程度地减少了所需的缓存空间，但带来了精度的下降。而 **分组查询注意力机制（中间，GQA）**，在精度和计算之间折中：将各查询 Q 分组，同组查询共享一个 KV 对，在保证精度的同时一定程度地减少缓存空间。

我们可以稍做计算：若我们将原有的 32 个注意力头分成 8 组，每组 4 个查询。那么就仅会有 8 个 KV 对，所需的缓存空间就变为了原来的四分之一（128GB -> 32GB）。由此，模型推理性能得到提升，我们就可以喂给模型更多和更长的句子，或者搭建更深的网络和设置更大的隐藏层维度。

metaAI 在代码中的具体实现方式为：限制 KV 头的数目小于 Q 头的数目，在计算的时候复制 KV 头以使其能够与 Q 的头对应，在缓存的时候仅缓存不重复的 KV 对。代码如下：

```python
# 重复 KV：x 是要重复的对象（K 或 V）
# n_rep 是重复的次数，它等于 Q 的头除以 KV 的头
def repeat_kv(x: torch.Tensor, n_rep: int) -> torch.Tensor:
    """torch.repeat_interleave(x, dim=2, repeats=n_rep)"""
    bs, slen, n_kv_heads, head_dim = x.shape
    if n_rep == 1:
        return x # 规定重复 1 次等于不重复
    return (
        x[:, :, :, None, :] # 重复后第 3 个维度变成 n_rep * KV 的头数
        .expand(bs, slen, n_kv_heads, n_rep, head_dim)
        .reshape(bs, slen, n_kv_heads * n_rep, head_dim)
    )

class Attention(nn.Module):
    def forward(self, x, ...):
        ... # 此处省略部分代码

        keys = self.cache_k[:bsz, : start_pos + seqlen]
        values = self.cache_v[:bsz, : start_pos + seqlen]

        # 重复 KV 的头以使之与 Q 的头对应
        keys = repeat_kv(keys, self.n_rep)
        values = repeat_kv(values, self.n_rep)

        ## 以下代码是注意力机制的具体计算过程
        # B 批量大小    L 序列长度
        # H 注意力头数  D 隐藏层维度数
        # C 已缓存的序列长度
        xq = xq.transpose(1, 2)  # [B, L, H, D] -> [B, H, L, D]
        keys = keys.transpose(1, 2) # [B, C + L, H, D] -> [B, H, C + L, D]
        values = values.transpose(1, 2) # [B, C + L, H, D] -> [B, H, C + L, D]
        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
        if mask is not None:
            scores = scores + mask  # [B, H, L, C + L]
        scores = F.softmax(scores.float(), dim=-1).type_as(xq)
        output = torch.matmul(scores, values) # [B, H, L, D]
        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
        return self.wo(output)
```

### CodeWork：回顾与总结，尝试阅读完整代码

在之前对 RoPE、KV-Cache、GQA 的介绍中，我们事实上已经将 LLaMA 中 Attention 模块的 forward() 函数部分的全部代码展示出来了。现在贴一份完整的代码在这里，可以试着在无注释的条件下理解代码做了什么。如果想看源码，也可以访问 [metaAI 官方的代码](https://github.com/meta-llama/llama/blob/main/llama/model.py)。

```python
class Attention(nn.Module):
    def forward(self, x: torch.Tensor, start_pos: int, freqs_cis: torch.Tensor, mask: Optional[torch.Tensor]):
        bsz, seqlen, _ = x.shape
        xq, xk, xv = self.wq(x), self.wk(x), self.wv(x)

        xq = xq.view(bsz, seqlen, self.n_local_heads, self.head_dim)
        xk = xk.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)
        xv = xv.view(bsz, seqlen, self.n_local_kv_heads, self.head_dim)

        xq, xk = apply_rotary_emb(xq, xk, freqs_cis=freqs_cis)

        self.cache_k = self.cache_k.to(xq)
        self.cache_v = self.cache_v.to(xq)

        self.cache_k[:bsz, start_pos : start_pos + seqlen] = xk
        self.cache_v[:bsz, start_pos : start_pos + seqlen] = xv

        keys = self.cache_k[:bsz, : start_pos + seqlen]
        values = self.cache_v[:bsz, : start_pos + seqlen]

        keys = repeat_kv(keys, self.n_rep)
        values = repeat_kv(values, self.n_rep)

        xq = xq.transpose(1, 2)
        keys = keys.transpose(1, 2)
        values = values.transpose(1, 2)
        scores = torch.matmul(xq, keys.transpose(2, 3)) / math.sqrt(self.head_dim)
        if mask is not None:
            scores = scores + mask
        scores = F.softmax(scores.float(), dim=-1).type_as(xq)
        output = torch.matmul(scores, values)
        output = output.transpose(1, 2).contiguous().view(bsz, seqlen, -1)
        return self.wo(output)
```

## 如何训练一个对话机器人：LLaMA2 是如何做的？

// to be continued

## 多模态？来看看 LLaMA3 的表现！

// to be continued
