---
layout: post
title: Llama1-3：从一道美团大模型面试题讲起🦘
date: 2025-01-09 20:25:53
categories:
    - 📖 论文阅读
tags: [大语言模型, NLP, NLP经典论文]
index_img: https://cdn.jsdelivr.net/gh/lazypool/blog-pics/animals/panda.png
---

# 从 LLaMA1 到 LLaMA3：这风云激荡的 2023 到 2024 年

这篇博客应当是我在 2025 年的首篇博客。24 年底，在我身上发生了很多意想不到的事情，总使我灰心丧气。忽然间，发觉自己除开写了十几个月的日记，并没有更多新的更新。前不久，在 B 站上看到名为 **美团大模型面试真题：LLaMA怎么优化注意力机制计算？** 的视频，我才惊觉时光飞逝。

于是想着，似乎应当写一篇详解 LLaMA 的博客出来。等到调查资料的时候，发现 LLaMA 原来早在 2023 年就已发布，且已到第三代。估计第四代就将在 2025 年发布吧？看来这篇博客实在是拖延不得了！
`（又：长久以来我总计划开设“NLP 经典论文”专区，整理自 2017 年以来“重点文”，不妨就以此篇作始吧！）`

## 2023 到 2024 年：从无到有，由弱及强

2023 到 2024 年，metaAI 先后发布 LLaMA1 到 LLaMA3 的论文。LLaMA 系列从无到有、由弱及强，成功跻身大语言模型先锋军之中。2023 也由此可以称作 **LLaMA 元年**。

- 2023.2.27  《LLaMA：开放高效的语言模型系列》 
- 2023.7.19  《LLaMA2：开放且经过微调的聊天模型》
- 2024.11.23 《LLaMA3 系列模型》

<table><thead>
    <tr><th rowspan="2">版本</th><th colspan="4" align="center">模型结构</th><th colspan="3" align="center">训练数据</th></tr>
    <tr><th>规范化</th><th>激活函数</th><th>位置编码</th><th>GQA</th><th>Tokens</th><th>上下文长度</th><th>Vocab Size</th></tr>
</thead><tbody>
    <tr><th>LLaMA1</th><th>RMSNorm</th><th>SwiGLU</th><th>RoPE</th><th>✗</th><th>1.4T</th><th>2K</th><th>32K</th></tr>
    <tr><th>LLaMA2</th><th>RMSNorm</th><th>SwiGLU</th><th>RoPE</th><th>✓</th><th>2.0T</th><th>4K</th><th>32K</th></tr>
    <tr><th>LLaMA3</th><th>RMSNorm</th><th>SiLU</th><th>RoPE</th><th>✓</th><th>15.0T</th><th>8K</th><th>128K</th></tr>
</tbody></table>

在模型结构上，LLaMA 自 1 代至 3 代总体改动较小，其中比较关键的技术是：**RMSNorm**、**RoPE** 和 **GQA**。**LLaMA 的迭代更新主要得力于 metaAI 不断扩充的优质语料库：**

- 自 1 代到 3 代，语料库的总 Token 量从 1.4T 上升至 15T；
- 词汇表大小也从最初的 32K 扩充至 128K；
- 预训练的上下文长度从 2K 增加到 4K，再到现在的 8K。

在模型参数量上，三代 LLaMA 总体呈现上升趋势：

- LLaMA1 分别具有 7B、13B、33B、65B 等版本；
- LLaMA2 与 1 代比差距不大，包括 7B、13B、34B、70B 等；
- LLaMA3 的参数量则显著提升，主要是 8B、70B 甚至 405B 等。

## LLaMA 是如何优化注意力机制的？

### RMSNorm：在不牺牲模型稳定性的同时降低计算复杂度

### RoPE：以简便方式有效捕捉相对位置信息

### GQA：减少 KV-Cache 大小以提升模型推理速度
