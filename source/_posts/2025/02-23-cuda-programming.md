---
layout: post
title: 从零入门 cuda 编程？🦴 看这篇就够了！
categories:
  - 💻 抠点代码
tags: [cuda 编程, 并行程序]
index_img: https://cdn.jsdelivr.net/gh/lazypool/blog-pics/animals/00012.jpg
date: 2025-02-23 22:12:23
---

# 实录：从零开始接触 cuda 编程 🐟

CUDA（全称 Compute Unified Device Architecture，中文译为“统一计算设备架构”），由 nvidia 在 2007 年发布，提供用于编程和管理 GPU 的 C/C++ 语言扩展和 API，服务于 GPU 通用计算，时至今日已历经十几年演进。本博客记录对 CUDA 的学习过程，重点内容包括：

1. CUDA 的架构详述，包括：硬件、软件和各种常见术语
2. CUDA 编程方法，包括：语法解释、

## 详解 CUDA 架构：软件层和硬件层相结合

在 CUDA 中，GPU 作为 CPU 的协处理器工作。通常，我们将 CPU 和 GPU 系统分别称为 **主机(host)** 和 **设备(device)** ，它们是具有各自内存空间的独立平台 📚。一般地，我们在 CPU 上运行串行工作负载，并将并行计算卸载到 GPU 上。

![CUDA 架构：软件层和硬件层](0223_cuda-architecture.png)

CUDA 的架构包括软件层和硬件层，如上图所示。软件层包括：**线程 (thread)**，**线程块 (block)** 和 **网格 (grid)** 。硬件层包括：**CUDA 核心** (或称流处理器，也即 SP)，**流式多处理器 (SM, streaming multiprocessor)** 和 **GPU**。除这些以外，还有 **线程束 (warp)** ，它充当沟通硬件层和软件层的桥梁。

### 线程和 CUDA 核心：最小的基本运算单位

> **线程 (thread)** 🧶

线程是对计算机中 **程序执行流** 的形象比喻，它由 **指令流** 和 **数据流** 交织构成，是 GPU/CPU 通用的原子级概念。在 CUDA 中，线程按束调度： **同束线程共享相同的程序计数器** ，它们同步地执行相同的指令，作用于各自寄存器存储的数据。

> **CUDA 核心 (SP)**

CUDA 核心是 CUDA 中执行标量运算指令的基本单元，其核心组件是 **整数运算单元 (INT)** 和 **浮点数运算单元 (FP)** 。CUDA 核心和线程相对应：**单个 CUDA 核心执行来自单个线程的指令** 。与线程相同，CUDA 核心同样按束调度： **同束核心在同一时刻执行相同的指令** ，但作业于不同的寄存器。

![CUDA 核心](0223_cuda-core.png)

> **单指令多线程 (SIMT, Single Instruction MultiThreads)**

🪶 这里，我们特别指出：上述线程和 CUDA 核心所采用的“ **多个线程在同一时刻执行相同的指令，但作用于不同的数据** ”的并行方式，即所谓的 **单指令多线程** ，这也是 CUDA 所使用的方式。与 SIMT 相对的是 SIMD (Single Instruction MultiData, 单指令多数据)，它更像是 vector 架构。

### 线程块、网格和 Kernel：CUDA 的概念模型

> **线程块 (block)** 🧊 和 **网格 (grid)** 🥅

线程块是对许多线程的概念抽象，它将数量繁多的线程按照 1 维、2 维或 3 维的方式组织，为开发者遍历、索引线程提供了极大方便。比线程块更高级的概念是网格，它将许多个线程块按照 1 维、2 维或 3 维的方式排列。由此，我们得到了 **网格-线程块-线程** 的线程层级概念划分。

> **线程坐标与索引的转换**

譬如下图左半部分，对于大小为 $(D_x, D_y)$ 二维线程块，其中索引为 $(x, y)$ 的线程的 **块内线程 ID** 应当是 $x + yD_x$ 。然而，由于线程块的维度和网格的维度能以各种方式组合，在不同组合下获取线程的全局 ID 或者块内 ID 仍然是一个复杂的问题。

> **CUDA 内存的读写规则**

CUDA 允许设备上的线程以 **寻址、共享、缓冲** 的方式读取 DRAM 和 On-Chip 内存。**更具体的，如下图右半部分，执行在设备上的线程，只允许按如下方式读写内存**：1. 读写每条线程的寄存器和本地内存。 2. 读写每个块的共享内存。 3. 读写每个网格的全局内存。 4. 只读每个网格的常量内存和纹理内存。

![Kernel 批处理（左）与 CUDA 内存模型（右）](https://blogger.googleusercontent.com/img/b/R29vZ2xl/AVvXsEgfwJ-XTFD3uN4CxFuFaUOOxF_YFA_1uUAVRWykrYSyrikT9ihmFyRyVXl-s7xZPnx1VGZTIln5MxL83fMearxY1fWc4RHQ7fbokHBgIJWTWi-lymFhYn3zRb64kk2PzugsJJVlzj1PoWI/s1600/gpu2.png)

> **Kernel (核函数)** 🥜

**特指由主机调用，在设备上运行的函数，它指示了网格内所有线程的行为。** 程序运行时，主机向设备连续地发送 kernel 调用的请求，每个 kernel 就作为一个由线程块组成的线程批处理来执行，如上图左半部分所示。单个 kernel 可能由多个线程块执行，线程块内的线程将 **共享某块内存，并在必要时同步**。

### 流式多处理器：并行计算的幕后功臣

> **SM (流式多处理器，Streaming Multiprocessor)**

以 Fermi 为例，如下图所示，单个完整的 SM 除了若干个 SP 还应该包括：指令缓存、线程束调度器和分派单元、寄存器文件、加载/存储单元队列、特殊功能单元队列、共享内存/L1 缓存、统一缓存等。它们的作用听名字都能猜出来，感兴趣可以自己去查一下，这里不再赘述 🐈🐕。

![流式多处理器 SM 架构图](./0223_cuda-sm-architecture.png)

### 线程束：连接软件层和硬件层的纽带

> **线程束 (warp) 🧵**

**线程束是真正执行并行操作的单位，通常以 32 个线程为一束。** 线程束是连接软、硬件层的纽带，软件层面开发者定义的线程块，通过线程束这一执行单元映射到硬件层面的 SM 上。具体来说，**每个线程块在加载到 SM 时，会被划分为若干束，而这些束正是 SM 的实际调度单位。**

> **线程束的并行与同步**

**同束线程共享内存，在运行时保持并行，仅在必要时同步。** 束内线程仅 32 个，因此同步的时间开销是可接受的。 **线程束的共享内存实际上作为缓存，用于缓存本束的计算结果，然后再上传。** 对所有束的计算结果同步汇总的时间也是可接受的。由此，也能知道 CUDA 计算的时间开销为：

$$总时间成本 = 核心计算的时间 + 束内线程同步的时间 + 对各个束进行同步的时间$$

> **线程束的分化**

**同束线程执行不同指令就叫做线程束分化。** 同束线程执行相同的指令，但处理各自的数据。若它们在执行时遇不同的控制条件，就会进行不同的选择，导致线程束分化。线程束分化严重影响性能。条件分支越多，并行性削弱越严重。因此，应尽量避免同束内线程分化，确保线程分配到线程束是有规律的。

![线程束分化示意图](./0223_cuda-warpbranches.png)

## 正式进入 CUDA 编程，源码到底怎么写？

我们花了大量的时间和篇幅详述 CUDA 的硬件/软件层面的架构，这有助于我们理解接下来的内容。CUDA 的编程主要围绕 Kernel 展开，我们将会逐步深入对它的理解。

### 下载 CUDA 库及编译工具

```bash
sudo pacman -S cuda
```

### 简单比较下 C 和 CUDA 程序

让我们从 Hello World 程序开始，简单比较一下 C 和 CUDA，理解下 CUDA 在实际编程上的特点。

<table><tbody><tr><td>

###### C

```cpp
void c_hello(){
    printf("Hello World!\n");
}

int main() {
    c_hello();
    return 0;
}
```
</td><td>

###### CPP

```cpp
__global__ void cuda_hello(){
    printf("Hello World from GPU!\n");
}

int main() {
    cuda_hello<<<1,1>>>(); 
    return 0;
}
```
</td></tr></tbody></table>

可以看到主要区别有两点，都是对于 Kernel 函数而言的：

1. 函数声明/定义时使用 `__global__` 限定符。
2. 调用函数时使用 `<<...>>` 语法指定执行配置。
