---
layout: post
title: ä»é›¶å…¥é—¨ cuda ç¼–ç¨‹ï¼ŸğŸ¦´ çœ‹è¿™ç¯‡å°±å¤Ÿäº†ï¼
categories:
  - ğŸ”§ å·¥å…·ä½¿ç”¨
tags: [cuda ç¼–ç¨‹, å¹¶è¡Œç¨‹åº]
index_img: https://cdn.jsdelivr.net/gh/lazypool/blog-pics/animals/00012.jpg
date: 2025-02-23 22:12:23
---

# å®å½•ï¼šä»é›¶å¼€å§‹æ¥è§¦ cuda ç¼–ç¨‹ ğŸŸ

CUDAï¼ˆå…¨ç§° Compute Unified Device Architectureï¼Œä¸­æ–‡è¯‘ä¸ºâ€œç»Ÿä¸€è®¡ç®—è®¾å¤‡æ¶æ„â€ï¼‰ï¼Œç”± nvidia åœ¨ 2007 å¹´å‘å¸ƒï¼Œæä¾›ç”¨äºç¼–ç¨‹å’Œç®¡ç† GPU çš„ C/C++ è¯­è¨€æ‰©å±•å’Œ APIï¼ŒæœåŠ¡äº GPU é€šç”¨è®¡ç®—ï¼Œæ—¶è‡³ä»Šæ—¥å·²å†ç»åå‡ å¹´æ¼”è¿›ã€‚æœ¬åšå®¢è®°å½•å¯¹ CUDA çš„å­¦ä¹ è¿‡ç¨‹ï¼Œé‡ç‚¹å†…å®¹åŒ…æ‹¬ï¼š

1. CUDA çš„æ¶æ„è¯¦è¿°ï¼ŒåŒ…æ‹¬ï¼šç¡¬ä»¶ã€è½¯ä»¶å’Œå„ç§å¸¸è§æœ¯è¯­ã€‚
2. CUDA çš„å®‰è£…ã€ç¼–è¯‘å·¥å…·å’Œç¼–è¾‘å™¨çš„é…ç½®ã€‚
3. CUDA çš„åŸºç¡€è¯­æ³•ï¼šå‡½æ•°æ ‡è¯†ç¬¦ã€å˜é‡æ ‡è¯†ç¬¦ã€å†…ç½®å‘é‡ç±»å‹å’Œå†…ç½®å˜é‡ã€æ‰§è¡Œé…ç½®ã€‚
3. CUDA çš„ Runtime APIï¼šå†…å­˜å’Œçº¿ç¨‹ç®¡ç†ã€è®¾å¤‡ç®¡ç†ã€äº‹ä»¶ç®¡ç†ã€æµç®¡ç†ã€‚
4. CUDA ç¼–ç¨‹å®ä¾‹ï¼šæµ‹è¯• GPU çš„ä¹˜åŠ æ€§èƒ½ã€‚

## è¯¦è§£ CUDA æ¶æ„ï¼šè½¯ä»¶å±‚å’Œç¡¬ä»¶å±‚ç›¸ç»“åˆ

åœ¨ CUDA ä¸­ï¼ŒGPU ä½œä¸º CPU çš„åå¤„ç†å™¨å·¥ä½œã€‚é€šå¸¸ï¼Œæˆ‘ä»¬å°† CPU å’Œ GPU ç³»ç»Ÿåˆ†åˆ«ç§°ä¸º **ä¸»æœº(host)** å’Œ **è®¾å¤‡(device)** ï¼Œå®ƒä»¬æ˜¯å…·æœ‰å„è‡ªå†…å­˜ç©ºé—´çš„ç‹¬ç«‹å¹³å° ğŸ“šã€‚ä¸€èˆ¬åœ°ï¼Œæˆ‘ä»¬åœ¨ CPU ä¸Šè¿è¡Œä¸²è¡Œå·¥ä½œè´Ÿè½½ï¼Œå¹¶å°†å¹¶è¡Œè®¡ç®—å¸è½½åˆ° GPU ä¸Šã€‚

![CUDA æ¶æ„ï¼šè½¯ä»¶å±‚å’Œç¡¬ä»¶å±‚](cuda-architecture.png)

CUDA çš„æ¶æ„åŒ…æ‹¬è½¯ä»¶å±‚å’Œç¡¬ä»¶å±‚ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºã€‚è½¯ä»¶å±‚åŒ…æ‹¬ï¼š**çº¿ç¨‹ (thread)**ï¼Œ**çº¿ç¨‹å— (block)** å’Œ **ç½‘æ ¼ (grid)** ã€‚ç¡¬ä»¶å±‚åŒ…æ‹¬ï¼š**CUDA æ ¸å¿ƒ** (æˆ–ç§°æµå¤„ç†å™¨ï¼Œä¹Ÿå³ SP)ï¼Œ**æµå¼å¤šå¤„ç†å™¨ (SM, streaming multiprocessor)** å’Œ **GPU**ã€‚é™¤è¿™äº›ä»¥å¤–ï¼Œè¿˜æœ‰ **çº¿ç¨‹æŸ (warp)** ï¼Œå®ƒå……å½“æ²Ÿé€šç¡¬ä»¶å±‚å’Œè½¯ä»¶å±‚çš„æ¡¥æ¢ã€‚

### çº¿ç¨‹å’Œ CUDA æ ¸å¿ƒï¼šæœ€å°çš„åŸºæœ¬è¿ç®—å•ä½

> **çº¿ç¨‹ (thread)** ğŸ§¶

çº¿ç¨‹æ˜¯å¯¹è®¡ç®—æœºä¸­ **ç¨‹åºæ‰§è¡Œæµ** çš„å½¢è±¡æ¯”å–»ï¼Œå®ƒç”± **æŒ‡ä»¤æµ** å’Œ **æ•°æ®æµ** äº¤ç»‡æ„æˆï¼Œæ˜¯ GPU/CPU é€šç”¨çš„åŸå­çº§æ¦‚å¿µã€‚åœ¨ CUDA ä¸­ï¼Œçº¿ç¨‹æŒ‰æŸè°ƒåº¦ï¼š **åŒæŸçº¿ç¨‹å…±äº«ç›¸åŒçš„ç¨‹åºè®¡æ•°å™¨** ï¼Œå®ƒä»¬åŒæ­¥åœ°æ‰§è¡Œç›¸åŒçš„æŒ‡ä»¤ï¼Œä½œç”¨äºå„è‡ªå¯„å­˜å™¨å­˜å‚¨çš„æ•°æ®ã€‚

> **CUDA æ ¸å¿ƒ (SP)**

CUDA æ ¸å¿ƒæ˜¯ CUDA ä¸­æ‰§è¡Œæ ‡é‡è¿ç®—æŒ‡ä»¤çš„åŸºæœ¬å•å…ƒï¼Œå…¶æ ¸å¿ƒç»„ä»¶æ˜¯ **æ•´æ•°è¿ç®—å•å…ƒ (INT)** å’Œ **æµ®ç‚¹æ•°è¿ç®—å•å…ƒ (FP)** ã€‚CUDA æ ¸å¿ƒå’Œçº¿ç¨‹ç›¸å¯¹åº”ï¼š**å•ä¸ª CUDA æ ¸å¿ƒæ‰§è¡Œæ¥è‡ªå•ä¸ªçº¿ç¨‹çš„æŒ‡ä»¤** ã€‚ä¸çº¿ç¨‹ç›¸åŒï¼ŒCUDA æ ¸å¿ƒåŒæ ·æŒ‰æŸè°ƒåº¦ï¼š **åŒæŸæ ¸å¿ƒåœ¨åŒä¸€æ—¶åˆ»æ‰§è¡Œç›¸åŒçš„æŒ‡ä»¤** ï¼Œä½†ä½œä¸šäºä¸åŒçš„å¯„å­˜å™¨ã€‚

![CUDA æ ¸å¿ƒ](cuda-core.png)

> **å•æŒ‡ä»¤å¤šçº¿ç¨‹ (SIMT, Single Instruction MultiThreads)**

ğŸª¶ è¿™é‡Œï¼Œæˆ‘ä»¬ç‰¹åˆ«æŒ‡å‡ºï¼šä¸Šè¿°çº¿ç¨‹å’Œ CUDA æ ¸å¿ƒæ‰€é‡‡ç”¨çš„â€œ **å¤šä¸ªçº¿ç¨‹åœ¨åŒä¸€æ—¶åˆ»æ‰§è¡Œç›¸åŒçš„æŒ‡ä»¤ï¼Œä½†ä½œç”¨äºä¸åŒçš„æ•°æ®** â€çš„å¹¶è¡Œæ–¹å¼ï¼Œå³æ‰€è°“çš„ **å•æŒ‡ä»¤å¤šçº¿ç¨‹** ï¼Œè¿™ä¹Ÿæ˜¯ CUDA æ‰€ä½¿ç”¨çš„æ–¹å¼ã€‚ä¸ SIMT ç›¸å¯¹çš„æ˜¯ SIMD (Single Instruction MultiData, å•æŒ‡ä»¤å¤šæ•°æ®)ï¼Œå®ƒæ›´åƒæ˜¯ vector æ¶æ„ã€‚

### çº¿ç¨‹å—ã€ç½‘æ ¼å’Œ Kernelï¼šCUDA çš„æ¦‚å¿µæ¨¡å‹

> **çº¿ç¨‹å— (block)** ğŸ§Š å’Œ **ç½‘æ ¼ (grid)** ğŸ¥…

çº¿ç¨‹å—æ˜¯å¯¹è®¸å¤šçº¿ç¨‹çš„æ¦‚å¿µæŠ½è±¡ï¼Œå®ƒå°†æ•°é‡ç¹å¤šçš„çº¿ç¨‹æŒ‰ç…§ 1 ç»´ã€2 ç»´æˆ– 3 ç»´çš„æ–¹å¼ç»„ç»‡ï¼Œä¸ºå¼€å‘è€…éå†ã€ç´¢å¼•çº¿ç¨‹æä¾›äº†æå¤§æ–¹ä¾¿ã€‚æ¯”çº¿ç¨‹å—æ›´é«˜çº§çš„æ¦‚å¿µæ˜¯ç½‘æ ¼ï¼Œå®ƒå°†è®¸å¤šä¸ªçº¿ç¨‹å—æŒ‰ç…§ 1 ç»´ã€2 ç»´æˆ– 3 ç»´çš„æ–¹å¼æ’åˆ—ã€‚ç”±æ­¤ï¼Œæˆ‘ä»¬å¾—åˆ°äº† **ç½‘æ ¼-çº¿ç¨‹å—-çº¿ç¨‹** çš„çº¿ç¨‹å±‚çº§æ¦‚å¿µåˆ’åˆ†ã€‚

> **çº¿ç¨‹åæ ‡ä¸ç´¢å¼•çš„è½¬æ¢**

è­¬å¦‚ä¸‹å›¾å·¦åŠéƒ¨åˆ†ï¼Œå¯¹äºå¤§å°ä¸º $(D_x, D_y)$ äºŒç»´çº¿ç¨‹å—ï¼Œå…¶ä¸­ç´¢å¼•ä¸º $(x, y)$ çš„çº¿ç¨‹çš„ **å—å†…çº¿ç¨‹ ID** åº”å½“æ˜¯ $x + yD_x$ ã€‚ç„¶è€Œï¼Œç”±äºçº¿ç¨‹å—çš„ç»´åº¦å’Œç½‘æ ¼çš„ç»´åº¦èƒ½ä»¥å„ç§æ–¹å¼ç»„åˆï¼Œåœ¨ä¸åŒç»„åˆä¸‹è·å–çº¿ç¨‹çš„å…¨å±€ ID æˆ–è€…å—å†… ID ä»ç„¶æ˜¯ä¸€ä¸ªå¤æ‚çš„é—®é¢˜ã€‚

> **CUDA å†…å­˜çš„è¯»å†™è§„åˆ™**

CUDA å…è®¸è®¾å¤‡ä¸Šçš„çº¿ç¨‹ä»¥ **å¯»å€ã€å…±äº«ã€ç¼“å†²** çš„æ–¹å¼è¯»å– DRAM å’Œ On-Chip å†…å­˜ã€‚**æ›´å…·ä½“çš„ï¼Œå¦‚ä¸‹å›¾å³åŠéƒ¨åˆ†ï¼Œæ‰§è¡Œåœ¨è®¾å¤‡ä¸Šçš„çº¿ç¨‹ï¼Œåªå…è®¸æŒ‰å¦‚ä¸‹æ–¹å¼è¯»å†™å†…å­˜**ï¼š1. è¯»å†™æ¯æ¡çº¿ç¨‹çš„å¯„å­˜å™¨å’Œæœ¬åœ°å†…å­˜ã€‚ 2. è¯»å†™æ¯ä¸ªå—çš„å…±äº«å†…å­˜ã€‚ 3. è¯»å†™æ¯ä¸ªç½‘æ ¼çš„å…¨å±€å†…å­˜ã€‚ 4. åªè¯»æ¯ä¸ªç½‘æ ¼çš„å¸¸é‡å†…å­˜å’Œçº¹ç†å†…å­˜ã€‚

![Kernel æ‰¹å¤„ç†ï¼ˆå·¦ï¼‰ä¸ CUDA å†…å­˜æ¨¡å‹ï¼ˆå³ï¼‰](cuda-kernels-memory.png)

> **Kernel (æ ¸å‡½æ•°)** ğŸ¥œ

**ç‰¹æŒ‡ç”±ä¸»æœºè°ƒç”¨ï¼Œåœ¨è®¾å¤‡ä¸Šè¿è¡Œçš„å‡½æ•°ï¼Œå®ƒæŒ‡ç¤ºäº†ç½‘æ ¼å†…æ‰€æœ‰çº¿ç¨‹çš„è¡Œä¸ºã€‚** ç¨‹åºè¿è¡Œæ—¶ï¼Œä¸»æœºå‘è®¾å¤‡è¿ç»­åœ°å‘é€ kernel è°ƒç”¨çš„è¯·æ±‚ï¼Œæ¯ä¸ª kernel å°±ä½œä¸ºä¸€ä¸ªç”±çº¿ç¨‹å—ç»„æˆçš„çº¿ç¨‹æ‰¹å¤„ç†æ¥æ‰§è¡Œï¼Œå¦‚ä¸Šå›¾å·¦åŠéƒ¨åˆ†æ‰€ç¤ºã€‚å•ä¸ª kernel å¯èƒ½ç”±å¤šä¸ªçº¿ç¨‹å—æ‰§è¡Œï¼Œçº¿ç¨‹å—å†…çš„çº¿ç¨‹å°† **å…±äº«æŸå—å†…å­˜ï¼Œå¹¶åœ¨å¿…è¦æ—¶åŒæ­¥**ã€‚

### æµå¼å¤šå¤„ç†å™¨ï¼šå¹¶è¡Œè®¡ç®—çš„å¹•ååŠŸè‡£

> **SM (æµå¼å¤šå¤„ç†å™¨ï¼ŒStreaming Multiprocessor)**

ä»¥ Fermi ä¸ºä¾‹ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œå•ä¸ªå®Œæ•´çš„ SM é™¤äº†è‹¥å¹²ä¸ª SP è¿˜åº”è¯¥åŒ…æ‹¬ï¼šæŒ‡ä»¤ç¼“å­˜ã€çº¿ç¨‹æŸè°ƒåº¦å™¨å’Œåˆ†æ´¾å•å…ƒã€å¯„å­˜å™¨æ–‡ä»¶ã€åŠ è½½/å­˜å‚¨å•å…ƒé˜Ÿåˆ—ã€ç‰¹æ®ŠåŠŸèƒ½å•å…ƒé˜Ÿåˆ—ã€å…±äº«å†…å­˜/L1 ç¼“å­˜ã€ç»Ÿä¸€ç¼“å­˜ç­‰ã€‚å®ƒä»¬çš„ä½œç”¨å¬åå­—éƒ½èƒ½çŒœå‡ºæ¥ï¼Œæ„Ÿå…´è¶£å¯ä»¥è‡ªå·±å»æŸ¥ä¸€ä¸‹ï¼Œè¿™é‡Œä¸å†èµ˜è¿° ğŸˆğŸ•ã€‚

![æµå¼å¤šå¤„ç†å™¨ SM æ¶æ„å›¾](cuda-sm-architecture.png)

### çº¿ç¨‹æŸï¼šè¿æ¥è½¯ä»¶å±‚å’Œç¡¬ä»¶å±‚çš„çº½å¸¦

> **çº¿ç¨‹æŸ (warp) ğŸ§µ**

**çº¿ç¨‹æŸæ˜¯çœŸæ­£æ‰§è¡Œå¹¶è¡Œæ“ä½œçš„å•ä½ï¼Œé€šå¸¸ä»¥ 32 ä¸ªçº¿ç¨‹ä¸ºä¸€æŸã€‚** çº¿ç¨‹æŸæ˜¯è¿æ¥è½¯ã€ç¡¬ä»¶å±‚çš„çº½å¸¦ï¼Œè½¯ä»¶å±‚é¢å¼€å‘è€…å®šä¹‰çš„çº¿ç¨‹å—ï¼Œé€šè¿‡çº¿ç¨‹æŸè¿™ä¸€æ‰§è¡Œå•å…ƒæ˜ å°„åˆ°ç¡¬ä»¶å±‚é¢çš„ SM ä¸Šã€‚å…·ä½“æ¥è¯´ï¼Œ**æ¯ä¸ªçº¿ç¨‹å—åœ¨åŠ è½½åˆ° SM æ—¶ï¼Œä¼šè¢«åˆ’åˆ†ä¸ºè‹¥å¹²æŸï¼Œè€Œè¿™äº›æŸæ­£æ˜¯ SM çš„å®é™…è°ƒåº¦å•ä½ã€‚**

> **çº¿ç¨‹æŸçš„å¹¶è¡Œä¸åŒæ­¥**

**åŒæŸçº¿ç¨‹å…±äº«å†…å­˜ï¼Œåœ¨è¿è¡Œæ—¶ä¿æŒåŒæ­¥ã€‚æŸä¸æŸä¹‹é—´åˆ™ä¿æŒå¹¶è¡Œï¼Œä»…åœ¨å¿…è¦æ—¶åŒæ­¥ã€‚** æŸå†…çº¿ç¨‹ä»… 32 ä¸ªï¼Œå› æ­¤åŒæ­¥çš„æ—¶é—´å¼€é”€æ˜¯å¯æ¥å—çš„ã€‚ **çº¿ç¨‹æŸçš„å…±äº«å†…å­˜å®é™…ä¸Šä½œä¸ºç¼“å­˜ï¼Œç”¨äºç¼“å­˜æœ¬æŸçš„è®¡ç®—ç»“æœï¼Œç„¶åå†ä¸Šä¼ ã€‚** å¯¹æ‰€æœ‰æŸçš„è®¡ç®—ç»“æœåŒæ­¥æ±‡æ€»çš„æ—¶é—´ä¹Ÿæ˜¯å¯æ¥å—çš„ã€‚ç”±æ­¤ï¼Œä¹Ÿèƒ½çŸ¥é“ CUDA è®¡ç®—çš„æ—¶é—´ï¼š

$$æ€»æ—¶é—´æˆæœ¬ = æ ¸å¿ƒè®¡ç®—çš„æ—¶é—´ + æŸå†…çº¿ç¨‹åŒæ­¥çš„æ—¶é—´ + å¯¹å„ä¸ªæŸè¿›è¡ŒåŒæ­¥çš„æ—¶é—´$$

> **çº¿ç¨‹æŸçš„åˆ†åŒ–**

**åŒæŸçº¿ç¨‹æ‰§è¡Œä¸åŒæŒ‡ä»¤å°±å«åšçº¿ç¨‹æŸåˆ†åŒ–ã€‚** åŒæŸçº¿ç¨‹æ‰§è¡Œç›¸åŒçš„æŒ‡ä»¤ï¼Œä½†å¤„ç†å„è‡ªçš„æ•°æ®ã€‚è‹¥å®ƒä»¬åœ¨æ‰§è¡Œæ—¶é‡ä¸åŒçš„æ§åˆ¶æ¡ä»¶ï¼Œå°±ä¼šè¿›è¡Œä¸åŒçš„é€‰æ‹©ï¼Œå¯¼è‡´çº¿ç¨‹æŸåˆ†åŒ–ã€‚**åˆ†åŒ–æœŸé—´ï¼Œæ¡ä»¶ä¸ºçœŸçš„çº¿ç¨‹å°†æ‰§è¡ŒæŒ‡ä»¤ï¼Œä¸ºå‡çš„çº¿ç¨‹åˆ™ç©ºç­‰ï¼ˆstall executionï¼‰ã€‚**çº¿ç¨‹æŸåˆ†åŒ–ä¸¥é‡å½±å“æ€§èƒ½ã€‚æ¡ä»¶åˆ†æ”¯è¶Šå¤šï¼Œå¹¶è¡Œæ€§å‰Šå¼±è¶Šä¸¥é‡ã€‚å› æ­¤ï¼Œåº”å°½é‡é¿å…åŒæŸå†…çº¿ç¨‹åˆ†åŒ–ï¼Œç¡®ä¿çº¿ç¨‹åˆ†é…åˆ°çº¿ç¨‹æŸæ˜¯æœ‰è§„å¾‹çš„ã€‚

![çº¿ç¨‹æŸåˆ†åŒ–ç¤ºæ„å›¾](cuda-warpbranches.png)

## æ­£å¼è¿›å…¥ CUDA ç¼–ç¨‹ï¼šå®‰è£…ã€ç¼–è¯‘å™¨ã€ç¼–è¾‘å™¨

æˆ‘ä»¬èŠ±äº†å¤§é‡çš„æ—¶é—´å’Œç¯‡å¹…è¯¦è¿° CUDA çš„è½¯ä»¶å±‚é¢å’Œç¡¬ä»¶å±‚é¢çš„æ¶æ„ï¼Œè¿™æœ‰åŠ©äºæˆ‘ä»¬ç†è§£æ¥ä¸‹æ¥çš„å†…å®¹ã€‚CUDA çš„ç¼–ç¨‹ä¸»è¦å›´ç»• Kernel å±•å¼€ï¼Œæˆ‘ä»¬å°†ä¼šé€æ­¥æ·±å…¥å¯¹å®ƒçš„ç†è§£ã€‚åœ¨æ­¤ä¹‹å‰ï¼Œå…ˆä¸‹è½½å¥½ CUDA çš„ç¼–è¯‘å™¨ nvccã€‚

### ä¸‹è½½ CUDA åº“åŠç¼–è¯‘å·¥å…· nvcc

```bash
# å®‰è£… cuda
sudo pacman -S cuda

# éªŒè¯å®‰è£…
nvcc --version # æ£€æŸ¥ CUDA ç¼–è¯‘å™¨
nvidia-smi     # æ£€æŸ¥ GPU é©±åŠ¨çŠ¶æ€
```

### é…ç½®ç¯å¢ƒå˜é‡

å°†ä»¥ä¸‹å†…å®¹æ·»åŠ åˆ° `~/.bashrc` æˆ– `~/.zshrc`ï¼š

```bash
export PATH="/opt/cuda/bin:$PATH"
export LD_LIBRARY_PATH="/opt/cuda/lib64:$LD_LIBRARY_PATH"
```

ç„¶åæ‰§è¡Œ `source ~/.bashrc` æˆ–é‡æ–°ç™»å½•ã€‚

### Neovim å¯ç”¨ CUDA è¯­æ³•æ£€æŸ¥

```lua
require("lspconfig").clangd.setup({
    capabilities = require("cmp_nvim_lsp").default_capabilities(),
    cmd = {
        "clangd",
        "--background-index",
        "--clang-tidy",
        "--header-insertion=iwyu",
        "--completion-style=detailed",
        "--query-driver=/usr/local/cuda/bin/nvcc", -- æŒ‡å®š CUDA ç¼–è¯‘å™¨è·¯å¾„
        "--offset-encoding=utf-16",
    },
    filetypes = { "c", "cpp", "cuda", "objc", "objcpp" }, -- æ·»åŠ  cuda ç±»å‹
})
```

## åŸºç¡€çš„ CUDA è¯­æ³•ï¼šC è¯­è¨€çš„è¿·ä½ æ‰©å±•é›†

è®©æˆ‘ä»¬ä» Hello World ç¨‹åºå¼€å§‹ï¼Œç®€å•æ¯”è¾ƒä¸€ä¸‹ C å’Œ CUDAï¼Œç†è§£ä¸‹ CUDA åœ¨å®é™…ç¼–ç¨‹ä¸Šçš„ç‰¹ç‚¹ã€‚

<table><tbody><tr><td>

**C/C++**

```cpp
void c_hello(){
    printf("Hello World!\n");
}

int main() {
    c_hello();
    return 0;
}
```
</td><td>

**CUDA**

```cpp
__global__ void cuda_hello(){
    printf("Hello World from GPU!\n");
}

int main() {
    cuda_hello<<<1,1>>>(); 
    return 0;
}
```
</td></tr></tbody></table>

å¯ä»¥çœ‹åˆ°ä¸»è¦åŒºåˆ«æœ‰ä¸¤ç‚¹ï¼šä¸€æ˜¯å‡½æ•°å£°æ˜/å®šä¹‰æ—¶ä½¿ç”¨ `__global__`ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä½œ **å‡½æ•°æ‰§è¡Œç©ºé—´æ ‡è¯†ç¬¦**ã€‚äºŒæ˜¯è°ƒç”¨å‡½æ•°æ—¶ä½¿ç”¨ `<<...>>`ï¼Œæˆ‘ä»¬å°†å…¶ç§°ä½œ **æ‰§è¡Œé…ç½®**ã€‚å®é™…ä¸Šï¼ŒCUDA å¯¹ C çš„æ‰©å±•å¯å½’çº³ä¸ºå››ç‚¹ï¼š1. å‡½æ•°æ‰§è¡Œç©ºé—´æ ‡è¯†ç¬¦ï¼›2. å˜é‡å†…å­˜ç©ºé—´æ ‡è¯†ç¬¦ï¼›3. å†…ç½®å‘é‡ç±»å‹å’Œå†…ç½®å˜é‡ï¼›4. æ‰§è¡Œé…ç½®ã€‚

### å‡½æ•°æ‰§è¡Œç©ºé—´æ ‡è¯†ç¬¦ (Function Execution Space Specifier)

#### `__global__`

1. å£°æ˜ä¸€ä¸ªå‡½æ•°ä¸º kernelï¼Œå®ƒåœ¨è®¾å¤‡ä¸Šæ‰§è¡Œï¼Œä»ä¸»æœºè°ƒç”¨ã€‚
2. `__global__` å‡½æ•°å¿…é¡»è¿”å› void ç±»å‹ï¼Œä¸”ä¸èƒ½ä½œä¸ºç±»æˆå‘˜æ–¹æ³•ã€‚
3. è°ƒç”¨ `__global__` å‡½æ•°å¿…é¡»ç»™å®šæ‰§è¡Œé…ç½®ã€‚
4. `__global__` çš„è°ƒç”¨æ˜¯å¼‚æ­¥çš„ï¼Œè¿™æ„å‘³ç€å®ƒä¼šåœ¨è®¾å¤‡æ‰§è¡Œå®Œæ¯•å‰è¿”å›ã€‚

#### `__device__`

1. å£°æ˜ä¸€ä¸ªå‡½æ•°ï¼Œåœ¨è®¾å¤‡ä¸Šæ‰§è¡Œï¼Œä¹Ÿä»…å¯ä»¥ä»è®¾å¤‡ä¸Šè°ƒç”¨ã€‚
2. å®ƒä¸èƒ½ä¸ `__global__` æ ‡è¯†ç¬¦åŒæ—¶ä½¿ç”¨ã€‚

#### `__host__`

1. å£°æ˜ä¸€ä¸ªå‡½æ•°ï¼Œåœ¨ä¸»æœºä¸Šæ‰§è¡Œï¼Œä¹Ÿä»…å¯ä»¥ä»ä¸»æœºä¸Šè°ƒç”¨ã€‚
2. é€šå¸¸è¯¥æ ‡è¯†ç¬¦å¯ä»¥çœç•¥ä¸å†™ï¼Œé»˜è®¤æ˜¯ä¸»æœºè°ƒç”¨å¹¶æ‰§è¡Œå‡½æ•°ã€‚
3. ä¸èƒ½ä¸ `__global__` æ ‡è¯†ç¬¦åŒæ—¶ä½¿ç”¨ã€‚
4. å½“ä¸ `__device__` æ ‡è¯†ç¬¦åŒæ—¶ä½¿ç”¨æ—¶ï¼Œä¼šåŒæ—¶ä¸ºä¸»æœºå’Œè®¾å¤‡ç¼–è¯‘è¯¥å‡½æ•°ã€‚

> Undefined behavior (æœªå®šä¹‰è¡Œä¸º)
> ä»¥ä¸‹å‡½æ•°è°ƒç”¨æ–¹å¼ä¼šè¢«ç¼–è¯‘å™¨è§£é‡Šä¸ºæœªå®šä¹‰è¡Œä¸ºï¼š
> 1. åœ¨ `__global__`, `__device__` æˆ– `__host__ __device__` å†…è°ƒç”¨ `__host__` å‡½æ•°ï¼›
> 2. åœ¨ `__host__` å‡½æ•°å†…è°ƒç”¨ `__device__` å‡½æ•°ã€‚

> å…¶ä»–æ³¨æ„äº‹é¡¹âš ï¸
> 1. `__device__` å’Œ `__global__` å‡½æ•°ä¸æ”¯æŒé€’å½’ã€‚
> 2. ä¸èƒ½åœ¨ `__device__` å’Œ `__global__` å‡½æ•°ä¸­å£°æ˜é™æ€å˜é‡ã€‚
> 3. `__device__` å’Œ `__global__` å‡½æ•°ä¸èƒ½æœ‰è‡ªå˜é‡çš„ä¸€ä¸ªå˜é‡æ•°å­—ã€‚
> 4. æ— æ³•è·å¾— `__device__` å‡½æ•°çš„åœ°å€ï¼Œä½†æŒ‡å‘ `__global__` æ˜¯åˆæ³•çš„ã€‚

### å˜é‡å†…å­˜ç©ºé—´æ ‡è¯†ç¬¦ (Variable Memory Space Specifiers)

> é»˜è®¤æƒ…å†µä¸‹ï¼ŒæœªæŒ‡å®šæ ‡è¯†ç¬¦çš„å˜é‡ä¼šé©»ç•™åœ¨å¯„å­˜å™¨ä¸Šã€‚ç„¶è€Œï¼Œéƒ¨åˆ†ç¼–è¯‘å™¨ä¼šå°†å…¶ç½®äºæœ¬åœ°å†…å­˜ï¼Œè¿™ä¼šå½±å“å¹¶è¡Œæ€§èƒ½ã€‚å› æ­¤ï¼Œåº”å½“å°½å¯èƒ½æŒ‡å®šå˜é‡çš„å†…å­˜ç©ºé—´æ ‡è¯†ç¬¦ã€‚

#### `__device__`

1. å£°æ˜ä¸€ä¸ªé©»ç•™åœ¨è®¾å¤‡å†…å­˜çš„å˜é‡ã€‚
2. å¯ä¸è‡³å¤š 1 ä¸ª<span class="hoverhint" data-hoverhint="__constant__  __shared__  __grid_constant__">å…¶ä»–çš„å˜é‡å†…å­˜æ ‡è¯†ç¬¦</span>ç»„åˆä½¿ç”¨ã€‚
3. é»˜è®¤è¡Œä¸º (å½“å®ƒå•ç‹¬ä½¿ç”¨æ—¶)ï¼š
    - ä½äºå…¨å±€å†…å­˜ç©ºé—´ã€‚
    - ç”Ÿå‘½å‘¨æœŸä¸åˆ›å»ºå®ƒçš„ CUDA ä¸Šä¸‹æ–‡ç›¸åŒã€‚
    - æ¯ä¸ªè®¾å¤‡æœ‰å…¶ç‹¬ç«‹çš„å‰¯æœ¬ã€‚
    - å¯é€šè¿‡<span class="hoverhint" data-hoverhint="cudaGetSymbolAddress()  cudaGetSymbolSize()  cudaMemcpyToSymbol()  cudaMemcpyFromSymbol()">è¿è¡Œæ—¶ API </span>è¢«ç½‘æ ¼å†…æ‰€æœ‰çº¿ç¨‹å’Œä¸»æœºè®¿é—®ã€‚
4. é€‚åˆå­˜å‚¨éœ€è¦å…¨å±€è®¿é—®çš„å¤§è§„æ¨¡æ•°æ®ã€‚

#### `__constant__`

1. å¿…é¡»ä¸ `__device__` ç»„åˆä½¿ç”¨ã€‚
2. é©»ç•™åœ¨å¸¸é‡å†…å­˜ç©ºé—´ã€‚
3. ç”Ÿå‘½å‘¨æœŸä¸åˆ›å»ºå®ƒçš„ CUDA ä¸Šä¸‹æ–‡ç›¸åŒã€‚
4. æ¯ä¸ªè®¾å¤‡æœ‰å…¶ç‹¬ç«‹çš„å‰¯æœ¬ã€‚
5. å¯é€šè¿‡<span class="hoverhint" data-hoverhint="cudaGetSymbolAddress()  cudaGetSymbolSize()  cudaMemcpyToSymbol()  cudaMemcpyFromSymbol()">è¿è¡Œæ—¶ API </span>è¢«ç½‘æ ¼å†…æ‰€æœ‰çº¿ç¨‹å’Œä¸»æœºè®¿é—®ã€‚
6. ä¸»æœºåœ¨å¹¶å‘å†…æ ¸è®¿é—®æ—¶ä¿®æ”¹å¸¸é‡ä¼šå¯¼è‡´æœªå®šä¹‰è¡Œä¸ºã€‚
7. é€‚åˆå­˜å‚¨åªè¯»æ•°æ®ã€‚

#### `__shared__`

1. å¿…é¡»ä¸ `__device__` ç»„åˆä½¿ç”¨ã€‚
2. é©»ç•™åœ¨çº¿ç¨‹å—çš„å…±äº«å†…å­˜ã€‚
3. ç”Ÿå‘½å‘¨æœŸä¸çº¿ç¨‹å—ç›¸åŒã€‚
4. æ¯ä¸ªçº¿ç¨‹å—æœ‰ç‹¬ç«‹å‰¯æœ¬ã€‚
5. ä»…å—å†…çº¿ç¨‹å¯è®¿é—®ï¼Œåœ°å€ä¸å›ºå®šã€‚

> åœ¨ CUDA ä¸­ï¼Œé€šè¿‡ `extern` å…³é”®å­—ï¼Œå¯ä½¿æ•°ç»„å¤§å°åœ¨å†…æ ¸å¯åŠ¨æ—¶ç”±æ‰§è¡Œé…ç½® `<<<...>>>` åŠ¨æ€æŒ‡å®šã€‚ä»¥è¿™ç§æ–¹å¼å£°æ˜çš„æ‰€æœ‰å˜é‡éƒ½ä»å†…å­˜ä¸­çš„ç›¸åŒåœ°å€å¼€å§‹ï¼Œå› æ­¤å¿…é¡»é€šè¿‡åç§»é‡æ˜¾å¼åœ°ç®¡ç†æ•°ç»„ä¸­å˜é‡çš„å¸ƒå±€ã€‚ä¾‹å¦‚ï¼Œåœ¨åŠ¨æ€åˆ†é…çš„å…±äº«å†…å­˜ä¸­ï¼Œå¯ä»¥ç”¨ä»¥ä¸‹æ–¹å¼å£°æ˜å’Œåˆå§‹åŒ–æ•°ç»„ï¼š

```cpp
extern __shared__ float array[];
__device__ void func() {     // __device__ or __global__ function
    short* array0 = (short*)array;     // å­˜å‚¨ short
    float* array1 = (float*)&array0[128]; // æ¥åœ¨ 128 ä¸ª short ä¹‹åï¼Œå­˜å‚¨ float
    int* array2 = (int*)&array1[64];    // æ¥åœ¨ 64 ä¸ª float ä¹‹åï¼Œå­˜å‚¨ int
}
```

> æ³¨æ„ï¼ŒæŒ‡é’ˆéœ€ä¸å…¶æ‰€æŒ‡å‘ç±»å‹å¯¹é½ï¼Œæ•…è€Œä»¥ä¸‹ä»£ç ä¸èƒ½å·¥ä½œï¼Œå› ä¸º `array1` æ²¡æœ‰å¯¹é½åˆ°å£°æ˜ `array` æ—¶æ‰€æŒ‡å®šçš„ float çš„ 4 å­—èŠ‚ã€‚

```cpp
extern __shared__ float array[];
__device__ void func() {     // __device__ or __global__ function
    short* array0 = (short*)array;
    float* array1 = (float*)&array0[127];   // é”™è¯¯ï¼127*2 å­—èŠ‚ä¸æ˜¯ 4 å­—èŠ‚å¯¹é½
}
```

### å†…ç½®å‘é‡ç±»å‹å’Œå†…ç½®å˜é‡

#### å†…ç½®å‘é‡ç±»å‹ (Built-in Vector Types)

<div style="display:flex;">
<div style="flex:1; width:60%; margin-right:10%; padding:1px;">

è¿™äº›æ˜¯ä»åŸºæœ¬æ•´æ•°å’Œæµ®ç‚¹ç±»å‹æ´¾ç”Ÿå‡ºæ¥çš„å‘é‡ç±»å‹ã€‚å®ƒä»¬æ˜¯ç»“æ„ä½“ï¼Œå…¶ç¬¬ 1ã€2ã€3 å’Œ 4 ä¸ªåˆ†é‡åˆ†åˆ«å¯ä»¥é€šè¿‡å­—æ®µ xã€yã€z å’Œ w æ¥è®¿é—®ã€‚å®ƒä»¬éƒ½å¸¦æœ‰ä¸€ä¸ªæ„é€ å‡½æ•°ï¼Œå½¢å¼ä¸º `make_<type name>`ï¼›ä¾‹å¦‚ï¼Œ

```cpp
int2 make_int2(int x, int y);
```

è¯¥æ„é€ å‡½æ•°ä¼šåˆ›å»ºä¸€ä¸ªå€¼ä¸º `(x, y)` çš„ `int2` ç±»å‹çš„å‘é‡ã€‚å‘é‡ç±»å‹çš„å¯¹é½è¦æ±‚åœ¨å³è¡¨ä¸­æœ‰è¯¦ç»†è¯´æ˜ã€‚
</div>
<div style="flex:2; width:30%;">
<table style="overflow-y:auto; height:20em;"><thead style="position:sticky;">
    <tr><th>ç±»å‹</th><th>å¯¹é½</th></tr>
</thead><tbody>
    <tr><td>char1, uchar1</td><td>1</td></tr>
    <tr><td>char2, uchar2</td><td>2</td></tr>
    <tr><td>char3, uchar3</td><td>1</td></tr>
    <tr><td>char4, uchar4</td><td>4</td></tr>
    <tr><td>short1, ushort1</td><td>2</td></tr>
    <tr><td>short2, ushort2</td><td>4</td></tr>
    <tr><td>short3, ushort3</td><td>2</td></tr>
    <tr><td>short4, ushort4</td><td>8</td></tr>
    <tr><td>int1, uint1</td><td>4</td></tr>
    <tr><td>int2, uint2</td><td>8</td></tr>
    <tr><td>int3, uint3</td><td>4</td></tr>
    <tr><td>int4, uint4</td><td>16</td></tr>
    <tr><td>long2, ulong2</td><td>8 or 16</td></tr>
    <tr><td>long3, ulong3</td><td>4 or 8</td></tr>
    <tr><td>long4, ulong4</td><td>16</td></tr>
    <tr><td>longlong1, ulonglong1</td><td>8</td></tr>
    <tr><td>longlong2, ulonglong2</td><td>16</td></tr>
    <tr><td>longlong3, ulonglong3</td><td>8</td></tr>
    <tr><td>longlong4, ulonglong4</td><td>16</td></tr>
    <tr><td>float1</td><td>4</td></tr>
    <tr><td>float2</td><td>8</td></tr>
    <tr><td>float3</td><td>4</td></tr>
    <tr><td>float4</td><td>16</td></tr>
    <tr><td>double1</td><td>8</td></tr>
    <tr><td>double2</td><td>16</td></tr>
    <tr><td>double3</td><td>8</td></tr>
    <tr><td>double4</td><td>16</td></tr>
    <tr><td>long1, ulong1</td><td>4 or 8</td></tr>
</tbody></table>
</div>
</div>

é™¤ä¸Šé¢å¸¸ç”¨çš„å‘é‡ç±»å‹ï¼ŒCUDA è¿˜æä¾›äº†åä¸º `dim3` çš„å‘é‡ç±»å‹ã€‚è¯¥ç±»å‹æ˜¯ä¸€ä¸ªåŸºäº `uint3` çš„æ•´æ•°å‘é‡ç±»å‹ï¼Œç”¨äºæŒ‡å®šçº¿ç¨‹å—å’Œç½‘æ ¼çš„å°ºå¯¸ï¼Œåœ¨åé¢è®²æ‰§è¡Œé…ç½®æ—¶æˆ‘ä»¬ä¼šçœ‹åˆ°å®ƒçš„ä½œç”¨ã€‚åœ¨å®šä¹‰ç±»å‹ä¸º `dim3` çš„å˜é‡æ—¶ï¼Œä»»ä½•æœªæŒ‡å®šçš„ç»„ä»¶éƒ½åˆå§‹åŒ–ä¸º 1ã€‚

#### å†…ç½®å˜é‡ (Built-in Variables)

- **gridDim**  <span style="margin-left:11px;">ï¼š</span>`dim3`ï¼Œè¡¨ç¤ºç½‘æ ¼çš„ç»´åº¦ `(gridDim.x, gridDim.y, gridDim.z)`ã€‚
- **blockIdx** <span style="margin-left: 6px;">ï¼š</span>`uint3`ï¼Œè¡¨ç¤ºç½‘æ ¼ä¸­å—çš„åæ ‡ `(blockIdx.x, blockIdx.y, blockIdx.z)`ã€‚
- **blockDim** <span style="margin-left: 0px;">ï¼š</span>`dim3`ï¼Œè¡¨ç¤ºçº¿ç¨‹å—çš„ç»´åº¦ `(blockDim.x, blockDim.y, blockDim.z)`ã€‚
- **threadIdx**<span style="margin-left: 5px;">ï¼š</span>`uint3`ï¼Œè¡¨ç¤ºçº¿ç¨‹åœ¨å—ä¸­çš„åæ ‡ `(threadIdx.x, threadIdx.y, threadIdx.z)`ã€‚
- **warpSize** <span style="margin-left: 3px;">ï¼š</span>`int` ï¼ŒåŒ…å«äº†ä»¥çº¿ç¨‹ä¸ºå•ä½çš„çº¿ç¨‹æ•°çš„å¤§å°ã€‚

### æ‰§è¡Œé…ç½® (Execution Configuration)

è°ƒç”¨ `__global__` å‡½æ•°å¿…é¡»æŒ‡å®šæ‰§è¡Œé…ç½®ã€‚æ‰§è¡Œé…ç½®å®šä¹‰äº†å°†åœ¨è®¾å¤‡ä¸Šæ‰§è¡Œè¯¥å‡½æ•°çš„ç½‘æ ¼å’Œå—çš„ç»´åº¦ï¼Œä»¥åŠç›¸å…³çš„ streamã€‚æ‰§è¡Œé…ç½®é€šè¿‡åœ¨å‡½æ•°åå’Œæ‹¬å·å‚æ•°åˆ—è¡¨ä¹‹é—´æ’å…¥ `<<< Dg, Db, Ns, S >>>` å½¢å¼çš„è¡¨è¾¾å¼æ¥æŒ‡å®šï¼Œå…¶ä¸­ï¼š

- **Dg** æ˜¯ `dim3` ç±»å‹ï¼ŒæŒ‡å®šç½‘æ ¼çš„ç»´åº¦å¤§å°ï¼Œä½¿å¾—å¯åŠ¨çš„çº¿ç¨‹å—çš„å—æ•°ä¸º `Dg.x * Dg.y * Dg.z`ã€‚
- **Db** åŒæ ·æ˜¯ `dim3` ç±»å‹ï¼ŒæŒ‡å®šæ¯ä¸ªå—çš„ç»´åº¦å¤§å°ï¼Œä½¿å¾—æ¯ä¸ªå—å…·æœ‰ `Db.x * Db.y * Db.z` ä¸ªçº¿ç¨‹ã€‚
- **Ns** æ˜¯ `size_t` ç±»å‹ï¼ŒæŒ‡å®šæ¯ä¸ªå—ä¸ºæ­¤è°ƒç”¨ **åŠ¨æ€åˆ†é…çš„å…±äº«å†…å­˜å­—èŠ‚æ•°** ã€‚æ­¤åŠ¨æ€åˆ†é…çš„å†…å­˜ç”¨äºä»»ä½•å£°æ˜ä¸º `extern` æ•°ç»„çš„å˜é‡ï¼ˆå¦‚ `__shared__` ä¸­æ‰€è¿°ï¼‰ï¼›`Ns` æ˜¯å¯é€‰å‚æ•°ï¼Œé»˜è®¤ä¸º 0ã€‚
- **S** æ˜¯ `cudaStream_t` ç±»å‹ï¼ŒæŒ‡å®šç›¸å…³çš„æµï¼›`S` æ˜¯ä¸€ä¸ªå¯é€‰å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º 0ã€‚

ä¾‹å¦‚ï¼Œå£°æ˜ä¸ºä»¥ä¸‹å½¢å¼çš„å‡½æ•°ï¼Œå¿…é¡»åƒè¿™æ ·è°ƒç”¨ï¼š

```cpp
__global__ void Func(float* parameter); // __global__ å£°æ˜å…¶ä¸º kernel å‡½æ•°

Func<<< Dg, Db, Ns >>>(parameter);  // è°ƒç”¨æ—¶å¿…é¡»æŒ‡å®šæ‰§è¡Œé…ç½®
```

- æ‰§è¡Œé…ç½®çš„å‚æ•°åœ¨å®é™…å‡½æ•°å‚æ•°ä¹‹å‰è¢«è¯„ä¼°ï¼Œä»¥ä¸‹ä¸¤ç§æƒ…å†µå°†å¯¼è‡´å‡½æ•°è°ƒç”¨å¤±è´¥ï¼š
    1. `Dg` æˆ– `Db` å¤§äºè®¾å¤‡å…è®¸çš„æœ€å¤§å¤§å°ï¼ˆå¦‚è®¡ç®—èƒ½åŠ›ä¸­æ‰€æŒ‡å®šï¼‰ï¼›
    2. `Ns` å¤§äºè®¾å¤‡ä¸Šå¯ç”¨çš„æœ€å¤§å…±äº«å†…å­˜é‡å‡å»é™æ€åˆ†é…æ‰€éœ€çš„å…±äº«å†…å­˜é‡ã€‚

## è¿›é˜¶ï¼šéƒ¨åˆ†å¸¸ç”¨çš„ Runtime API

### å†…å­˜ä¸çº¿ç¨‹ç®¡ç†

#### cudaMalloc()

> cudaError_t cudaMalloc(void** devPtr, size_t count);

- åœ¨è®¾å¤‡ä¸Šåˆ†é… count å­—èŠ‚çš„çº¿æ€§å†…å­˜ï¼Œå¹¶å°†æŒ‡å‘è¯¥éƒ¨åˆ†å†…å­˜çš„æŒ‡é’ˆäº¤ç»™ devPtrã€‚
- åˆ†é…çš„å†…å­˜é€‚åˆä»»ä½•ç±»å‹çš„å˜é‡ã€‚
- å¦‚æœåˆ†é…å¤±è´¥ï¼Œè¿”å› `cudaErrorAlloction`ã€‚

#### cudaFree()

> cudaError_t cudaFree(void* devPtr);

- é‡Šæ”¾è¢« devPtr æŒ‡å‘çš„å†…å­˜ç©ºé—´ã€‚
- devPtr å¿…é¡»è¢« cudaMalloc() èµ‹å€¼è¿‡ï¼Œå¦åˆ™ä¼šå¼•å‘é”™è¯¯ã€‚
- cudaFree() å¯ä»¥é‡å¤è°ƒç”¨ï¼Œè¿™å°†ä¸ä¼šæ‰§è¡Œä»»ä½•æ“ä½œã€‚

#### cudaMemcpy()

> cudaError_t cudaMemcpy(void* dst, const void* src, size_t count, enum cudaMemcpyKind kind);

> cudaError_t cudaMemcpyAsync(void* dst,constvoid*src, size_t count, enum cudaMemcpyKind kind, cudaStream_t stream);

- æ‹·è´ count å­—èŠ‚ï¼Œä» src æŒ‡å‘çš„å†…å­˜åŒºåŸŸåˆ° dst æŒ‡å‘çš„å†…å­˜åŒºåŸŸã€‚
- kind å¯ä»¥æ˜¯ `cudaMemcpyHostToHost`ï¼Œ `cudaMemcpyHostToDevice`ï¼Œ`cudaMemcpyDeviceToHost`ï¼Œæˆ– `cudaMemcpyDeviceToDevice` çš„æ‹·è´æ–¹å‘ã€‚
- `cudaMemcpyAsync()` æ˜¯å¼‚æ­¥çš„ï¼Œå¹¶ä¸”å¯ä»¥ä½œä¸ºä¸€ä¸ªå¯é€‰å‚æ•°é€šè¿‡æµä½¿ç”¨ã€‚

#### cudaDeviceSynchronize()

> cudaError_t cudaDeviceSynchronize(void);

- é˜»å¡ï¼Œç›´åˆ°è®¾å¤‡ä¸Šæ‰€æœ‰è¯·æ±‚çš„ä»»åŠ¡æ‰§è¡Œå®Œæ¯•ã€‚
- è‹¥ä»»ä½•ä¸€ä¸ªä»»åŠ¡å¤±è´¥ï¼Œ`cudaThreadSynchronize()` éƒ½å°†è¿”å›ä¸€ä¸ªé”™è¯¯ã€‚

#### ç¤ºä¾‹ï¼šç®¡ç†è®¾å¤‡å†…å­˜å’Œç­‰å¾…è®¾å¤‡æ‰§è¡Œ

```cpp
# include <stdio.h>

int main() {
    // å£°æ˜ host data å’Œ device data
    // å¤§å°ä¸º 10 ä¸ª float
    float *h_data, *d_data;
    size_t size = 10 * sizeof(float);

    // åˆå§‹åŒ– host data
    h_data = (float*)malloc(size)
    for (int i = 0; i < size; i++) {
        h_data[i] = 3.14 + i % 10;
    }

    cudaMalloc(&d_data, size); // 1. ä¸º device data å¼€è¾Ÿç©ºé—´
    cudaMemcpy(d_data, h_data, size, cudaMemcpyHostToDevice); // 2. å°† host data æ‹·è´è‡³ device data

    myKernel<<<...>>>(); // 3. æ‰§è¡Œæ ¸å‡½æ•°

    cudaDeviceSynchronize(); // 4. é˜»å¡ï¼Œç›´åˆ°è®¾å¤‡æ‰§è¡Œå®Œæ¯•
}
```

### è®¾å¤‡ç®¡ç† (Device)

#### cudaGetDeviceCount()

> cudaError_t cudaGetDeviceCount(int* count);

- å°†è®¡ç®—å…¼å®¹æ€§å¤§äºç­‰äº 1.0 çš„è®¾å¤‡æ•°é‡èµ‹å€¼åˆ° count æ‰€æŒ‡å‘çš„ int å˜é‡ã€‚
- å¦‚æœæ²¡æœ‰ç›¸å…³è®¾å¤‡ï¼Œè¿”å› 1ã€‚

#### cudaSetDevice()

> cudaError_t cudaSetDevice(int dev);

- æŒ‡ç¤ºå½“å‰æ‰€å¤„çš„ä¸»æœºçº¿ç¨‹ï¼Œä½¿ç”¨ä»£ç ä¸º dev çš„è®¾å¤‡ã€‚

#### cudaGetDevice()

> cudaError_t cudaGetDevice(int* dev);

- å°†å½“å‰ä¸»æœºçº¿ç¨‹æ‰€ç”¨è®¾å¤‡çš„ä»£ç èµ‹å€¼åˆ° dev æ‰€æŒ‡å‘çš„ int å˜é‡ã€‚

#### cudaGetDeviceProperties()

> cudaError_t cudaGetDeviceProperties(struct cudaDeviceProp* prop, int dev);

- å°†ä»£ç ä¸º dev çš„è®¾å¤‡çš„å±æ€§èµ‹å€¼åˆ° prop æ‰€æŒ‡å‘çš„ cudaDeviceProp ç»“æ„ä½“ã€‚
- cudaDeviceProp ç»“æ„ä½“çš„å®šä¹‰å¯å‚é˜… [nvidia æ–‡æ¡£](https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html)

#### ç¤ºä¾‹ï¼šè·æ‚‰è®¾å¤‡å±æ€§

```cpp
#include <stdio.h>

int main() {
  int device_count;
  cudaGetDeviceCount(&device_count);  // 1. è·å–è®¾å¤‡æ•°é‡
  printf("Found %d CUDA devices\n", device_count);

  cudaSetDevice(0);  // 2. é€‰æ‹© 0 å·è®¾å¤‡

  int dev;
  cudaGetDevice(&int);  // 3. è·å–å½“å‰çš„è®¾å¤‡ç¼–å·

  cudaDeviceProp prop;
  cudaGetDeviceProperties(&prop, 0);  // 4. è·å–è®¾å¤‡å±æ€§
  printf("Device Name: %s\n", prop.name);
  printf("Compute Capability: %d.%d\n", prop.major, prop.minor);
  
  return 0;
}
```

### äº‹ä»¶ç®¡ç† (Event)

#### cudaEventCreate()

> cudaError_t cudaEventCreate(cudaEvent_t* eventPtr);

- åˆ›å»ºä¸€ä¸ªäº‹ä»¶å¯¹è±¡ï¼Œå®ƒå…·æœ‰ cudaEvent_t ç±»å‹ï¼Œè¢« eventPtr æŒ‡é’ˆæ‰€æŒ‡å‘ã€‚
- cudaEvent_t ç±»å‹çš„å®šä¹‰å¯å‚è€ƒ [nvidia æ–‡æ¡£](https://docs.nvidia.com/cuda/cuda-runtime-api/cudaEvent_t.html)

#### cudaEventRecord()

> cudaError_t cudaEventRecord(cudaEvent_t event, CUstream stream);

- è®°å½•äº‹ä»¶ eventï¼Œæ³¨æ„æ­¤å¤„çš„ event å¹¶éæŒ‡é’ˆï¼Œè€Œæ˜¯ cudaEvent_t ç±»å‹ã€‚
- å¦‚æœstream æ˜¯éé›¶çš„ï¼Œå½“æµä¸­æ‰€æœ‰çš„æ“ä½œå®Œæ¯•ï¼Œäº‹ä»¶è¢«è®°å½•ã€‚
- è‹¥ stream ä¸ºç©ºï¼Œå½“CUDA context ä¸­æ‰€æœ‰çš„æ“ä½œå®Œæ¯•ï¼Œäº‹ä»¶è¢«è®°å½•ã€‚
- è¯¥æ“ä½œæ˜¯å¼‚æ­¥çš„ï¼Œå¿…é¡»ä½¿ç”¨ `cudaEventSyncronize()` æ¥ç¡®ä¿äº‹ä»¶çœŸçš„è¢«è®°å½•ã€‚

#### cudaEventSyncronize()

> cudaError_t cudaEventSyncronize(cudaEvent_t event);

- é˜»å¡çº¿ç¨‹ï¼Œç›´åˆ°äº‹ä»¶ event çœŸçš„è¢«è®°å½•ã€‚
- å¦‚æœ `cudaEventRecord()` ä»æœªè¢«è°ƒç”¨è¿‡ï¼Œåˆ™è¿”å› `cudaErrorInvalidValue`ã€‚

#### cudaEventElapsedTime()

> cudaError_t cudaEventElapsedTime(float* time, cudaEvent_t start, cudaEvent_t end);

- è®¡ç®—ä¸¤ä¸ªäº‹ä»¶ start å’Œ end ä¹‹é—´æ‰€èŠ±è´¹çš„æ—¶é—´ï¼Œå¹¶èµ‹å€¼åˆ° time æ‰€æŒ‡çš„ float å˜é‡ã€‚
- start å’Œ end å¿…é¡»éƒ½è¢«è°ƒç”¨è¿‡ `cudaEventRecord()`ï¼Œå¹¶ä¸”å·²ç»çœŸçš„è¢«è®°å½•äº†ã€‚
- å¦åˆ™ï¼Œå°†è¿”å› `cudaErrorInvalidValue` é”™è¯¯ã€‚
- æ­¤å¤–ï¼Œè‹¥ start å’Œ end ä¸­æœ‰è¢«å¸¦éé›¶ stream è®°å½•çš„äº‹ä»¶ï¼Œç»“æœå°†ä¼šæ˜¯ undefinedã€‚

#### cudaEventDestroy()

> cudaError_t cudaEventDestroy(cudaEvent_t event);

- é”€æ¯äº‹ä»¶å¯¹è±¡ eventã€‚

#### ç¤ºä¾‹ï¼šæµ‹é‡å†…æ ¸æ‰§è¡Œæ—¶é—´

```cpp
// 1. åˆ›å»ºäº‹ä»¶å¯¹è±¡
cudaEvent_t start, stop;
cudaEventCreate(&start);
cudaEventCreate(&stop);

cudaEventRecord(start); // 2. è®°å½•èµ·å§‹æ—¶é—´
myKernel<<<...>>>();  // æ‰§è¡Œå†…æ ¸
cudaEventRecord(stop); // 3. è®°å½•ç»ˆæ­¢æ—¶é—´

cudaEventSynchronize(stop); // 4. é˜»å¡ï¼Œç›´åˆ° stop è¢«è®°å½•
float milliseconds = 0;
cudaEventElapsedTime(&milliseconds, start, stop); // 5. è®¡ç®—æ—¶é—´

// 6. é”€æ¯äº‹ä»¶å¯¹è±¡
cudaEventDestroy(start);
cudaEventDestroy(stop);
```

### æµç®¡ç† (stream)

#### cudaStreamCreate()

> cudaError_t cudaStreamCreate(cudaStream_t* streamPtr);

- åˆ›å»ºä¸€ä¸ªæµå¯¹è±¡ï¼Œå®ƒæ˜¯ cudaStream_t ç±»å‹ï¼Œè¢« streamPtr æ‰€æŒ‡å‘ã€‚
- cudaStream_t ç±»å‹çš„å®šä¹‰å¯å‚è€ƒ [nvidia æ–‡æ¡£](https://docs.nvidia.com/cuda/cuda-runtime-api/cudaStream_t.html)

#### cudaStreamSyncronize()

> cudaError_t cudaStreamSyncronize(cudaStream_t stream);

- é˜»å¡å½“å‰è¿›ç¨‹ï¼Œç›´åˆ°è®¾å¤‡å®Œæˆæµ stream ä¸­çš„æ‰€æœ‰æ“ä½œã€‚

#### cudaStreamDestroy()

> cudaError_t cudaStreamDestroy(cudaStream_t stream);

- é”€æ¯ä¸€ä¸ªæµå¯¹è±¡ã€‚

#### ç¤ºä¾‹ï¼šå¼‚æ­¥æ•°æ®ä¼ è¾“

```cpp
// 1. åˆ›å»ºæµå¯¹è±¡
cudaStream_t stream;
cudaStreamCreate(&stream);

// å¼‚æ­¥å†…å­˜æ‹·è´
cudaMemcpyAsync(d_data, h_data, size, cudaMemcpyHostToDevice, stream);

// 2. å¼‚æ­¥æ‰§è¡Œå†…æ ¸
myKernel<<<grid, block, 0, stream>>>();

cudaStreamSynchronize(stream);  // 3. é˜»å¡ï¼Œç›´åˆ°æµå®Œæˆ
cudaStreamDestroy(stream); // 4. é”€æ¯æµå¯¹è±¡
```

## CUDA ç¼–ç¨‹å®ä¾‹ï¼šæµ‹è¯• GPU çš„ä¹˜åŠ æ€§èƒ½

åˆ©ç”¨ cuda ç¼–ç¨‹æµ‹è¯• GPU è¿›è¡Œä¹˜åŠ æµ®ç‚¹è¿ç®— `c+=a*b` çš„æ€§èƒ½ï¼Œå°†å…¶é‡åŒ–æˆ GFLOPS æŒ‡æ ‡ _(GFLOPS, Giga FLoating-point Operations Per Secondï¼Œæ¯ç§’ 10 äº¿æ¬¡çš„æµ®ç‚¹è¿ç®—æ•°)_ ï¼Œå¹¶å‘ˆç° GPU çš„éƒ¨åˆ†å±æ€§ã€‚æ‹Ÿå¯¹ $2^{24}$ ä¸ªå…ƒç´ è¿›è¡Œè®¡ç®—ï¼Œè¿­ä»£ 100 æ¬¡ã€‚

### Cuda æºä»£ç 

```cpp
#include <stdio.h>
#include <cuda_runtime.h>

#define CHECK(cmd) { \
	cudaError_t error = cmd; \
	if (error != cudaSuccess) { \
		printf("Error: %s:%d, ", __FILE__, __LINE__); \
		printf("code:%d, reason:%s\n", error, cudaGetErrorString(error)); \
		exit(1); \
	} \
}

__global__ void multiplyAddKernel(float *a, float *b, float *c, int n) {
	int i = blockIdx.x * blockDim.x + threadIdx.x;
	if (i < n) {
		c[i] += a[i] * b[i]; // ä¹˜åŠ æ“ä½œï¼šc += a * b
	}
}

int main() {
	// è·å–GPUå±æ€§
	cudaDeviceProp prop;
	CHECK(cudaGetDeviceProperties(&prop, 0));
	
	// æ‰“å°GPUå±æ€§
	printf("GPU å±æ€§:\n");
	printf("è®¾å¤‡åç§°: %s\n", prop.name);
	printf("è®¡ç®—èƒ½åŠ›: %d.%d\n", prop.major, prop.minor);
	printf("SM æ•°é‡: %d\n", prop.multiProcessorCount);
	printf("å…¨å±€å†…å­˜: %.2f GB\n", prop.totalGlobalMem / (1024.0 * 1024.0 * 1024.0));
	printf("æ¯å—æœ€å¤§çº¿ç¨‹æ•°: %d\n", prop.maxThreadsPerBlock);
	printf("æ¯çº¿ç¨‹å—å…±äº«å†…å­˜: %zu KB\n\n", prop.sharedMemPerBlock / 1024);

	// è®¾ç½®æ•°æ®é‡
	const int N = 1 << 24; // 16,777,216 ä¸ªå…ƒç´ 
	const size_t size = N * sizeof(float);
	const int blockSize = 256;
	const int gridSize = (N + blockSize - 1) / blockSize;
	const int iterations = 100; // é‡å¤æ‰§è¡Œæ¬¡æ•°

	// åˆ†é…ä¸»æœºå†…å­˜å¹¶åˆå§‹åŒ–
	float *h_a = (float*)malloc(size);
	float *h_b = (float*)malloc(size);
	float *h_c = (float*)malloc(size);
	for (int i = 0; i < N; ++i) {
		h_a[i] = 2.0f;
		h_b[i] = 0.5f;
		h_c[i] = 0.0f;
	}

	// åˆ†é…è®¾å¤‡å†…å­˜
	float *d_a, *d_b, *d_c;
	CHECK(cudaMalloc(&d_a, size));
	CHECK(cudaMalloc(&d_b, size));
	CHECK(cudaMalloc(&d_c, size));

	// æ‹·è´æ•°æ®åˆ°è®¾å¤‡
	CHECK(cudaMemcpy(d_a, h_a, size, cudaMemcpyHostToDevice));
	CHECK(cudaMemcpy(d_b, h_b, size, cudaMemcpyHostToDevice));
	CHECK(cudaMemcpy(d_c, h_c, size, cudaMemcpyHostToDevice));

	// é¢„çƒ­è¿è¡Œ
	for (int i = 0; i < 5; ++i) {
		multiplyAddKernel<<<gridSize, blockSize>>>(d_a, d_b, d_c, N);
		CHECK(cudaGetLastError());
	}
	CHECK(cudaDeviceSynchronize());

	// åˆ›å»ºè®¡æ—¶äº‹ä»¶
	cudaEvent_t start, stop;
	CHECK(cudaEventCreate(&start));
	CHECK(cudaEventCreate(&stop));

	// é‡ç½®ç»“æœä¸ºåˆå§‹å€¼
	CHECK(cudaMemcpy(d_c, h_c, size, cudaMemcpyHostToDevice));

	// æ‰§è¡Œå¹¶è®¡æ—¶
	CHECK(cudaEventRecord(start));
	for (int i = 0; i < iterations; ++i) {
		multiplyAddKernel<<<gridSize, blockSize>>>(d_a, d_b, d_c, N);
		CHECK(cudaGetLastError());
	}
	CHECK(cudaEventRecord(stop));
	CHECK(cudaEventSynchronize(stop));

	// è®¡ç®—æ—¶é—´
	float milliseconds;
	CHECK(cudaEventElapsedTime(&milliseconds, start, stop));
	double seconds = milliseconds / 1000.0;

	// è®¡ç®—æ€§èƒ½æŒ‡æ ‡
	double totalFlops = 2.0 * N * iterations; // æ¯æ¬¡è¿­ä»£æ¯ä¸ªå…ƒç´ 2æ¬¡æµ®ç‚¹æ“ä½œ
	double gflops = totalFlops / seconds / 1e9;

	printf("æ€§èƒ½æŒ‡æ ‡:\n");
	printf("æ•°æ®é‡: %d ä¸ªå…ƒç´ \n", N);
	printf("æ€»è¿ç®—é‡: %.2f GFLOP\n", totalFlops / 1e9);
	printf("æ€»è€—æ—¶: %.3f ms\n", milliseconds);
	printf("å¹³å‡æ€§èƒ½: %.2f GFLOPS\n\n", gflops);

	// éªŒè¯ç»“æœæ­£ç¡®æ€§
	CHECK(cudaMemcpy(h_c, d_c, size, cudaMemcpyDeviceToHost));
	printf("éªŒè¯å‰5ä¸ªç»“æœ:\n");
	for (int i = 0; i < 5; ++i) {
		printf("c[%d] = %.1f (é¢„æœŸå€¼: %d)\n", i, h_c[i], iterations);
	}

	// é‡Šæ”¾èµ„æº
	free(h_a);
	free(h_b);
	free(h_c);
	CHECK(cudaFree(d_a));
	CHECK(cudaFree(d_b));
	CHECK(cudaFree(d_c));
	CHECK(cudaEventDestroy(start));
	CHECK(cudaEventDestroy(stop));

	return 0;
}
```

### ç¼–è¯‘åŠè¾“å‡º

```bash
$ nvcc -ccbin g++ -m64 -gencode arch=compute_35,code=sm_35 a.cu

$ ./a.out
GPU å±æ€§:
è®¾å¤‡åç§°: Tesla K20c
è®¡ç®—èƒ½åŠ›: 3.5
SM æ•°é‡: 13
å…¨å±€å†…å­˜: 4.63 GB
æ¯å—æœ€å¤§çº¿ç¨‹æ•°: 1024
æ¯çº¿ç¨‹å—å…±äº«å†…å­˜: 48 KB

æ€§èƒ½æŒ‡æ ‡:
æ•°æ®é‡: 16777216 ä¸ªå…ƒç´ 
æ€»è¿ç®—é‡: 3.36 GFLOP
æ€»è€—æ—¶: 181.626 ms
å¹³å‡æ€§èƒ½: 18.47 GFLOPS

éªŒè¯å‰ 5 ä¸ªç»“æœ:
c[0] = 100.0 (é¢„æœŸå€¼: 100)
c[1] = 100.0 (é¢„æœŸå€¼: 100)
c[2] = 100.0 (é¢„æœŸå€¼: 100)
c[3] = 100.0 (é¢„æœŸå€¼: 100)
c[4] = 100.0 (é¢„æœŸå€¼: 100)
```

å¯ä»¥çœ‹åˆ°ï¼ŒTesla K20c æœ‰ 13 å— SMï¼Œæ¯å—æœ€å¤§çº¿ç¨‹æ•°ä¸º 1024ï¼Œæ˜¾å­˜ 4.63 GBï¼Œæ¯ç§’å¯æ‰§è¡Œ 184.7 äº¿æ¬¡æµ®ç‚¹è¿ç®—ã€‚åˆ°æ­¤ï¼Œæˆ‘ä»¬å·²ç»æŒæ¡äº†å…¥é—¨ CUDA æ‰€éœ€çš„å…¨éƒ¨åŸºç¡€çŸ¥è¯†äº†ï¼é¼“æŒğŸ‘ğŸ‘ğŸ‘
